import spaces
import os
import sys
import gradio as gr
import json
import re
import numpy as np
from datetime import datetime
from huggingface_hub import login
from cached_path import cached_path
import tempfile
import soundfile as sf
import contextlib

from gradio.themes.utils import colors, fonts

# --- Theme (Kh√¥ng ƒë·ªïi) ---
latte = gr.themes.Citrus(
    primary_hue=colors.yellow,
    secondary_hue=colors.rose,
    neutral_hue=colors.gray,
    font=fonts.GoogleFont("Inter"),
    font_mono=fonts.GoogleFont("JetBrains Mono")
).set(
    body_background_fill="#eff1f5",
    body_text_color="#4c4f69",
    border_color_primary="#bcc0cc",
    block_background_fill="#ffffff",
    button_primary_background_fill="#df8e1d",
    button_primary_text_color="#ffffff",
    link_text_color="#dc8a78",
)

# --- Ph·∫ßn t·∫£i model (Kh√¥ng ƒë·ªïi) ---
MODEL_TYPE = os.getenv('MODEL_TYPE', 'new')
CKPT_PATH = os.getenv('CKPT_HF_PATH')
VOCAB_PATH = os.getenv('VOCAB_HF_PATH')
WORD_LIMIT = int(os.getenv('WORD_LIMIT', 1000))
DISPLAY_NAME = os.getenv('DISPLAY_NAME', 'Unknown Model')
INIT_PARAMS_JSON = os.getenv('INIT_PARAMS_JSON', '{}')
INIT_PARAMS = json.loads(INIT_PARAMS_JSON)

PATH_TO_OLD_F5_REPO = os.path.abspath('f5_tts_old')
PATH_TO_NEW_F5_REPO = os.path.abspath('f5_tts_new')

if not CKPT_PATH or not VOCAB_PATH:
    raise ValueError("L·ªói: CKPT_PATH ho·∫∑c VOCAB_PATH ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p.")

tts_instance = None
print(f"Nh·∫≠n di·ªán lo·∫°i model: {MODEL_TYPE}")

if MODEL_TYPE == 'old':
    print("ƒêang t·∫£i model theo ki·∫øn tr√∫c F5-TTS Base...")
    sys.path.insert(0, PATH_TO_OLD_F5_REPO)
    from f5_tts.model import DiT
    from f5_tts.infer.utils_infer import load_vocoder, load_model, chunk_text
    vocoder = load_vocoder()
    model = load_model(
        DiT, dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4),
        ckpt_path=str(cached_path(CKPT_PATH)), vocab_file=str(cached_path(VOCAB_PATH)),
    )
    tts_instance = {"model": model, "vocoder": vocoder}
    sys.path.pop(0)
    print("‚úÖ T·∫£i model C≈® th√†nh c√¥ng.")
elif MODEL_TYPE == 'new':
    print(f"ƒêang t·∫£i model v·ªõi c√°c tham s·ªë: {INIT_PARAMS}")
    sys.path.insert(0, PATH_TO_NEW_F5_REPO)
    from f5tts_wrapper import F5TTSWrapper
    INIT_PARAMS['ckpt_path'] = str(cached_path(CKPT_PATH))
    INIT_PARAMS['vocab_file'] = str(cached_path(VOCAB_PATH))
    tts_instance = F5TTSWrapper(**INIT_PARAMS)
    sys.path.pop(0)
    print("‚úÖ T·∫£i model M·ªöI th√†nh c√¥ng.")

# Cache ri√™ng cho model c≈©
ref_audio_processed_old, ref_text_processed_old = None, None

# Helper ƒë·ªÉ ·∫©n output kh√¥ng c·∫ßn thi·∫øt
@contextlib.contextmanager
def suppress_outputs(target_path):
    original_stdout, original_stderr = sys.stdout, sys.stderr
    sys.path.insert(0, target_path)
    try:
        with open(os.devnull, 'w') as devnull:
            sys.stdout, sys.stderr = devnull, devnull
            yield
    finally:
        sys.stdout, sys.stderr = original_stdout, original_stderr
        sys.path.pop(0)

@spaces.GPU
def process_reference_audio(ref_audio_orig: str, progress=gr.Progress()):
    if not ref_audio_orig: return None, ""
    
    progress(0, desc="ƒêang x·ª≠ l√Ω gi·ªçng m·∫´u...")
    gr.Info("ƒê√£ nh·∫≠n audio m·∫´u. B·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")
    
    processed_path = ref_audio_orig
    transcribed_text = ""

    if MODEL_TYPE == 'new':
        with suppress_outputs(PATH_TO_NEW_F5_REPO):
            processed_path, transcribed_text = tts_instance.preprocess_reference(ref_audio_path=ref_audio_orig, ref_text="", clip_short=True)
    else:
        with suppress_outputs(PATH_TO_OLD_F5_REPO):
            from f5_tts.infer.utils_infer import preprocess_ref_audio_text
            global ref_audio_processed_old, ref_text_processed_old
            ref_audio_processed_old, ref_text_processed_old = preprocess_ref_audio_text(ref_audio_orig, "")
            transcribed_text = ref_text_processed_old
            processed_path = ref_audio_orig 

    progress(1, desc="X·ª≠ l√Ω gi·ªçng m·∫´u ho√†n t·∫•t!")
    gr.Info("X·ª≠ l√Ω gi·ªçng m·∫´u ho√†n t·∫•t! Vui l√≤ng ki·ªÉm tra v√† s·ª≠a l·∫°i vƒÉn b·∫£n phi√™n √¢m n·∫øu c·∫ßn.")
    print(f"X·ª≠ l√Ω gi·ªçng m·∫´u ho√†n t·∫•t. VƒÉn b·∫£n nh·∫≠n d·∫°ng: '{transcribed_text}'")
    
    return processed_path, transcribed_text

@spaces.GPU
def infer_tts(ref_audio_path: str, ref_text_ui: str, gen_text: str, speed: float, cfg_strength: float, nfe_step: int, force_reprocess: bool, progress=gr.Progress()):
    global ref_audio_processed_old, ref_text_processed_old
    if not ref_audio_path: raise gr.Error("L·ªói: Kh√¥ng t√¨m th·∫•y √¢m thanh m·∫´u. Vui l√≤ng t·∫£i l√™n.")
    if not gen_text.strip(): raise gr.Error("Vui l√≤ng nh·∫≠p n·ªôi dung vƒÉn b·∫£n.")
    
    try:
        ref_text_to_use = ref_text_ui.strip()

        if MODEL_TYPE == 'new':
            text_provided_and_changed = (ref_text_to_use != "" and ref_text_to_use != tts_instance.ref_text)
            should_reprocess_new = (force_reprocess or tts_instance.ref_audio_processed is None or text_provided_and_changed)

            if should_reprocess_new:
                progress(0.1, desc="ƒêang x·ª≠ l√Ω l·∫°i gi·ªçng m·∫´u...")
                print(f"ƒêang x·ª≠ l√Ω l·∫°i gi·ªçng m·∫´u v·ªõi vƒÉn b·∫£n: '{ref_text_to_use if ref_text_to_use else 'S·∫Ω ch·∫°y ASR'}'")
                with suppress_outputs(PATH_TO_NEW_F5_REPO):
                    tts_instance.preprocess_reference(ref_audio_path=ref_audio_path, ref_text=ref_text_to_use, clip_short=True)
                print("X·ª≠ l√Ω l·∫°i gi·ªçng m·∫´u ho√†n t·∫•t.")
            else:
                print("S·ª≠ d·ª•ng gi·ªçng m·∫´u ƒë√£ ƒë∆∞·ª£c cache b√™n trong model. B·ªè qua b∆∞·ªõc x·ª≠ l√Ω l·∫°i.")
        
        else: # Logic cho model c≈©
            should_reprocess_old = (force_reprocess or ref_audio_processed_old is None or (ref_text_to_use != "" and ref_text_to_use != ref_text_processed_old))
            if should_reprocess_old:
                progress(0.1, desc="ƒêang x·ª≠ l√Ω l·∫°i gi·ªçng m·∫´u...")
                with suppress_outputs(PATH_TO_OLD_F5_REPO):
                    from f5_tts.infer.utils_infer import preprocess_ref_audio_text
                    ref_audio_processed_old, ref_text_processed_old = preprocess_ref_audio_text(ref_audio_path, ref_text_to_use)
                print("X·ª≠ l√Ω l·∫°i gi·ªçng m·∫´u ho√†n t·∫•t.")
        
        print(f"B·∫Øt ƒë·∫ßu t·∫°o audio cho to√†n b·ªô vƒÉn b·∫£n...")
        
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
            tmp_path = tmp_wav.name

        if MODEL_TYPE == 'old':
            with suppress_outputs(PATH_TO_OLD_F5_REPO):
                from vinorm import TTSnorm
                from f5_tts.infer.utils_infer import infer_process, chunk_text
                sentences = chunk_text(TTSnorm(gen_text).lower())
                audio_chunks = []
                final_sr = 24000
                for i, sentence in enumerate(sentences):
                    progress(i / len(sentences), desc=f"ƒêang t·∫°o chunk {i+1}/{len(sentences)}")
                    wave, sr, _ = infer_process(ref_audio_processed_old, ref_text_processed_old.lower(), sentence, tts_instance["model"], tts_instance["vocoder"], speed=speed, nfe_step=nfe_step)
                    audio_chunks.append(wave)
                    final_sr = sr
                full_audio = np.concatenate(audio_chunks)
                sf.write(tmp_path, full_audio, final_sr)
        else: # Model new
            with suppress_outputs(PATH_TO_NEW_F5_REPO):
                from vinorm import TTSnorm
                final_text = TTSnorm(gen_text)
                tts_instance.generate(text=final_text, output_path=tmp_path, nfe_step=nfe_step, cfg_strength=cfg_strength, speed=speed, progress_callback=progress)

        final_wave, final_sr = sf.read(tmp_path)
        os.remove(tmp_path)
        
        output_dir = "output"
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        final_output_path = os.path.join(output_dir, f"generated_audio_{timestamp}.wav")
        sf.write(final_output_path, final_wave, final_sr)
        print(f"‚úÖ √Çm thanh ho√†n ch·ªânh ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {final_output_path}")
        
        progress(1, desc="Ho√†n th√†nh!")
        
        # <<< S·ª¨A L·ªñI: Lu√¥n l·∫•y ƒë∆∞·ªùng d·∫´n t·ª´ "ngu·ªìn ch√¢n l√Ω" trong wrapper >>>
        processed_ref_path_for_ui = tts_instance.last_processed_audio_path if MODEL_TYPE == 'new' else ref_audio_path

        return (final_sr, final_wave), None, processed_ref_path_for_ui

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise gr.Error(f"L·ªói khi t·∫°o gi·ªçng n√≥i: {e}")

# --- Giao di·ªán Gradio ---
with gr.Blocks(theme=latte) as demo:
    gr.Markdown("# üé§ F5-TTS: Vietnamese Text-to-Speech")
    gr.Markdown(f"### Model: **{DISPLAY_NAME}** | Ki·∫øn tr√∫c: **{MODEL_TYPE.upper()}** | Gi·ªõi h·∫°n: **{WORD_LIMIT} t·ª´**")
    with gr.Row():
        with gr.Column(scale=1):
            ref_audio_ui = gr.Audio(label="1. T·∫£i l√™n √¢m thanh m·∫´u", type="filepath")
            ref_text_ui = gr.Textbox(label="2. S·ª≠a l·∫°i vƒÉn b·∫£n (n·∫øu c·∫ßn)", placeholder="N·ªôi dung audio m·∫´u s·∫Ω t·ª± ƒë·ªông xu·∫•t hi·ªán ·ªü ƒë√¢y sau khi b·∫°n t·∫£i file l√™n.", lines=5, interactive=True)
        with gr.Column(scale=2):
            gen_text_ui = gr.Textbox(label="3. Nh·∫≠p vƒÉn b·∫£n c·∫ßn t·∫°o gi·ªçng n√≥i", placeholder="Nh·∫≠p vƒÉn b·∫£n d√†i v√†o ƒë√¢y...", lines=11)
    with gr.Row():
        speed_slider = gr.Slider(0.3, 2.0, value=1.0, step=0.1, label="‚ö° T·ªëc ƒë·ªô")
        cfg_strength_slider = gr.Slider(0.5, 5.0, value=2.5, step=0.1, label="üó£Ô∏è ƒê·ªô b√°m s√°t gi·ªçng m·∫´u")
    with gr.Accordion("üõ†Ô∏è C√†i ƒë·∫∑t n√¢ng cao", open=False):
        nfe_step_slider = gr.Slider(minimum=16, maximum=64, value=32, step=2, label="üîç S·ªë b∆∞·ªõc kh·ª≠ nhi·ªÖu (NFE)", info="Cao h∆°n = ch·∫≠m h∆°n nh∆∞ng c√≥ th·ªÉ ch·∫•t l∆∞·ª£ng t·ªët h∆°n. Th·∫•p h∆°n = nhanh h∆°n.")
        force_reprocess_ui = gr.Checkbox(label="B·∫Øt bu·ªôc x·ª≠ l√Ω l·∫°i gi·ªçng m·∫´u", value=False, info="Tick v√†o √¥ n√†y n·∫øu b·∫°n mu·ªën model h·ªçc l·∫°i gi·ªçng m·∫´u t·ª´ ƒë·∫ßu.")
    btn = gr.Button("üî• 4. T·∫°o gi·ªçng n√≥i", variant="primary")
    with gr.Row():
        output_audio = gr.Audio(label="üéß √Çm thanh t·∫°o ra", type="numpy")
        output_spectrogram = gr.Image(label="üìä Spectrogram (Ch·ªâ c√≥ ·ªü model c≈©)", visible=False)

    ref_audio_ui.upload(process_reference_audio, inputs=[ref_audio_ui], outputs=[ref_audio_ui, ref_text_ui])
    
    btn.click(
        infer_tts, 
        inputs=[ref_audio_ui, ref_text_ui, gen_text_ui, speed_slider, cfg_strength_slider, nfe_step_slider, force_reprocess_ui], 
        outputs=[output_audio, output_spectrogram, ref_audio_ui]
    )

demo.queue().launch(share=True)