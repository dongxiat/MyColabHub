import spaces
import os
import sys
import gradio as gr
import json
import numpy as np
from datetime import datetime
from huggingface_hub import login
from cached_path import cached_path
import tempfile
import soundfile as sf
import contextlib
import torch

from gradio.themes.utils import colors, fonts

# --- Theme ---
latte = gr.themes.Citrus(primary_hue=colors.yellow, secondary_hue=colors.rose, neutral_hue=colors.gray, font=fonts.GoogleFont("Inter"), font_mono=fonts.GoogleFont("JetBrains Mono")).set(body_background_fill="#eff1f5", body_text_color="#4c4f69", border_color_primary="#bcc0cc", block_background_fill="#ffffff", button_primary_background_fill="#df8e1d", button_primary_text_color="#ffffff", link_text_color="#dc8a78")

# --- C·∫§U H√åNH MODEL ---
MODELS_DATA = {
    "Hynt_ViVoice": {"display_name": "Hynt ViVoice (1000h ƒë·ªçc Ti·∫øng Vi·ªát Vivoice)", "repo": "hynt/F5-TTS-Vietnamese-ViVoice", "model_file": "model_last.pt", "vocab_file": "config.json", "type": "old"},
    "Hynt_100h": {"display_name": "Hynt (150h ƒë·ªçc Ti·∫øng Vi·ªát 500k Steps)", "repo": "cuongdesign/Vietnamese-TTS", "model_file": "model_500000.pt", "vocab_file": "vocab.txt", "type": "old"},
    "DanhTran": {"display_name": "DanhTran (100h ƒë·ªçc ti·∫øng Vi·ªát d·ªØ li·ªáu c·ªßa VinAI)", "repo": "danhtran2mind/vi-f5-tts", "model_file": "ckpts/model_last.pt", "vocab_file": "vocab.txt", "type": "old"},
    "ZaloPay": {"display_name": "ZaloPay (model c·ªßa Zalopay 1tr3 Steps)", "repo": "zalopay/vietnamese-tts", "model_file": "model_1290000.pt", "vocab_file": "vocab.txt", "type": "old"},
    "EraX Smile Female": {"display_name": "EraX Smile Female (Chuy√™n Clone cho gi·ªçng n·ªØ 8 v√πng mi·ªÅn)", "repo": "erax-ai/EraX-Smile-Female-F5-V1.0", "model_file": "model/model_612000.safetensors", "vocab_file": "model/vocab.txt", "type": "new", "init_params": {"vocoder_name": "vocos", "use_ema": False}},
    "EraX Smile Unisex": {"display_name": "EraX Smile Unisex (c·∫£ Nam/N·ªØ 8 v√πng mi·ªÅn)", "repo": "erax-ai/EraX-Smile-UnixSex-F5", "model_file": "models/overfit.safetensors", "vocab_file": "models/vocab.txt", "type": "new", "init_params": {"model_name": "F5TTS_v1_Base", "vocoder_name": "vocos", "use_ema": True, "target_sample_rate": 24000, "n_mel_channels": 100, "hop_length": 256, "win_length": 1024, "n_fft": 1024, "ode_method": 'euler'}},
}
MODEL_DISPLAY_NAMES = [info['display_name'] for info in MODELS_DATA.values()]
MODEL_DISPLAY_NAMES.insert(0, "(Ch∆∞a ch·ªçn model)")


# --- T·∫£i Gi·ªçng M·∫´u C√≥ S·∫µn ---
SAMPLES_DIR = "samples"
SAMPLES_CONFIG = os.path.join(SAMPLES_DIR, "samples.json")
def load_samples():
    sample_names = ["(T·ª± t·∫£i l√™n gi·ªçng c·ªßa b·∫°n)"]
    sample_lookup = {}
    if os.path.exists(SAMPLES_CONFIG):
        try:
            with open(SAMPLES_CONFIG, 'r', encoding='utf-8') as f:
                data = json.load(f)
            for sample in data:
                if 'name' in sample and 'audio_path' in sample and 'ref_text' in sample:
                    sample_names.append(sample['name'])
                    sample_lookup[sample['name']] = sample
            print(f"‚úÖ ƒê√£ t·∫£i th√†nh c√¥ng {len(sample_lookup)} gi·ªçng m·∫´u.")
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi ƒë·ªçc file samples.json: {e}")
    else:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file samples.json. B·ªè qua vi·ªác t·∫£i gi·ªçng m·∫´u c√≥ s·∫µn.")
    return sample_names, sample_lookup
sample_names, sample_lookup = load_samples()

# --- BI·∫æN TO√ÄN C·ª§C ƒê·ªÇ QU·∫¢N L√ù TR·∫†NG TH√ÅI ---
PATH_TO_OLD_F5_REPO = os.path.abspath('f5_tts_old')
PATH_TO_NEW_F5_REPO = os.path.abspath('f5_tts_new')
tts_instance = None
MODEL_TYPE = None
ref_audio_path_old, ref_text_processed_old = None, None

# --- C√ÅC H√ÄM X·ª¨ L√ù LOGIC ---

@contextlib.contextmanager
def suppress_outputs(target_path):
    original_stdout, original_stderr = sys.stdout, sys.stderr
    sys.path.insert(0, target_path)
    try:
        with open(os.devnull, 'w') as devnull:
            sys.stdout, sys.stderr = devnull, devnull
            yield
    finally:
        sys.stdout, sys.stderr = original_stdout, original_stderr
        sys.path.pop(0)

@spaces.GPU
def load_and_prepare_model(selected_display_name, progress=gr.Progress()):
    progress(0, desc="ƒêang t√¨m th√¥ng tin model...")
    global tts_instance, MODEL_TYPE, ref_audio_path_old, ref_text_processed_old

    if selected_display_name == "(Ch∆∞a ch·ªçn model)":
        tts_instance = None
        MODEL_TYPE = None
        return "Vui l√≤ng ch·ªçn m·ªôt model ƒë·ªÉ b·∫Øt ƒë·∫ßu.", None, "", gr.update(visible=False)

    tts_instance = None
    MODEL_TYPE = None
    ref_audio_path_old, ref_text_processed_old = None, None
    torch.cuda.empty_cache()

    try:
        model_key = next(key for key, info in MODELS_DATA.items() if info["display_name"] == selected_display_name)
        model_info = MODELS_DATA[model_key]
        
        MODEL_TYPE = model_info['type']
        
        progress(0.1, desc="ƒêang t·∫£i file t·ª´ Hugging Face Hub...")
        ckpt_path = str(cached_path(f"hf://{model_info['repo']}/{model_info['model_file']}"))
        vocab_path = str(cached_path(f"hf://{model_info['repo']}/{model_info['vocab_file']}"))
        
        progress(0.5, desc=f"ƒêang kh·ªüi t·∫°o model {selected_display_name}...")
        
        if MODEL_TYPE == 'old':
            print("ƒêang t·∫£i model theo ki·∫øn tr√∫c F5-TTS Base...")
            with suppress_outputs(PATH_TO_OLD_F5_REPO):
                from f5_tts.model import DiT
                from f5_tts.infer.utils_infer import load_vocoder, load_model
                vocoder = load_vocoder()
                model = load_model(
                    model_cls=DiT, 
                    model_cfg=dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4),
                    ckpt_path=ckpt_path, vocab_file=vocab_path
                )
                tts_instance = {"model": model, "vocoder": vocoder}
        elif MODEL_TYPE == 'new':
            print(f"ƒêang t·∫£i model v·ªõi c√°c tham s·ªë: {model_info.get('init_params', {})}")
            with suppress_outputs(PATH_TO_NEW_F5_REPO):
                from f5tts_wrapper import F5TTSWrapper
                from safetensors.torch import load_file
                init_params = model_info.get('init_params', {})
                init_params['ckpt_path'] = ckpt_path
                init_params['vocab_file'] = vocab_path
                tts_instance = F5TTSWrapper(**init_params)
        
        progress(1, desc="T·∫£i model th√†nh c√¥ng!")
        print(f"‚úÖ T·∫£i model {selected_display_name} th√†nh c√¥ng.")
        
        return f"‚úÖ Model ƒë√£ ƒë∆∞·ª£c t·∫£i: {selected_display_name}", None, "", gr.update(visible=(MODEL_TYPE == 'old'))
    
    except Exception as e:
        import traceback; traceback.print_exc()
        tts_instance, MODEL_TYPE = None, None
        error_msg = f"L·ªói khi t·∫£i model: {e}"
        gr.Error(error_msg)
        return error_msg, None, "", gr.update(visible=False)

def handle_preprocess(audio_path, text, clip_short, progress):
    progress(0, desc="ƒêang x·ª≠ l√Ω gi·ªçng m·∫´u...")
    gr.Info("ƒê√£ nh·∫≠n audio m·∫´u. B·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")
    
    if MODEL_TYPE == 'new':
        with suppress_outputs(PATH_TO_NEW_F5_REPO):
            processed_path, transcribed_text = tts_instance.preprocess_reference(
                ref_audio_path=audio_path, ref_text=text, clip_short=clip_short
            )
    else:
        global ref_audio_path_old, ref_text_processed_old
        with suppress_outputs(PATH_TO_OLD_F5_REPO):
            from f5_tts.infer.utils_infer import preprocess_ref_audio_text
            processed_path, transcribed_text = preprocess_ref_audio_text(audio_path, text, clip_short=clip_short)
            ref_audio_path_old = processed_path
            ref_text_processed_old = transcribed_text
    
    progress(1, desc="X·ª≠ l√Ω gi·ªçng m·∫´u ho√†n t·∫•t!")
    gr.Info("X·ª≠ l√Ω gi·ªçng m·∫´u ho√†n t·∫•t!")
    print(f"X·ª≠ l√Ω gi·ªçng m·∫´u ho√†n t·∫•t.\n ƒê∆∞·ªùng d·∫´n: {processed_path},\n VƒÉn b·∫£n c·ªßa gi·ªçng m·∫´u: '{transcribed_text}'")
    return processed_path, transcribed_text

@spaces.GPU
def select_sample(sample_name, progress=gr.Progress()):
    if tts_instance is None:
        raise gr.Error("Vui l√≤ng ch·ªçn v√† t·∫£i m·ªôt model tr∆∞·ªõc khi ch·ªçn gi·ªçng m·∫´u.")
    if not sample_name or sample_name == sample_names[0]: return None, ""
    sample_data = sample_lookup[sample_name]
    audio_path = os.path.join(SAMPLES_DIR, sample_data['audio_path'])
    return handle_preprocess(audio_path, sample_data['ref_text'], clip_short=False, progress=progress)

@spaces.GPU
def process_manual_upload(ref_audio_orig, progress=gr.Progress()):
    if tts_instance is None:
        raise gr.Error("Vui l√≤ng ch·ªçn v√† t·∫£i m·ªôt model tr∆∞·ªõc khi t·∫£i l√™n gi·ªçng m·∫´u.")
    if not ref_audio_orig: return None, "", sample_names[0]
    processed_path, transcribed_text = handle_preprocess(ref_audio_orig, "", clip_short=True, progress=progress)
    return processed_path, transcribed_text, sample_names[0]

@spaces.GPU
def infer_tts(ref_audio_path, ref_text_from_ui, gen_text, speed, cfg_strength, nfe_step, output_path_from_ui, output_volume, pause_duration, progress=gr.Progress()):
    if tts_instance is None: raise gr.Error("L·ªói: Vui l√≤ng ch·ªçn v√† t·∫£i m·ªôt model tr∆∞·ªõc khi t·∫°o gi·ªçng n√≥i.")
    is_ready = (tts_instance.ref_audio_processed is not None) if MODEL_TYPE == 'new' else (ref_audio_path_old is not None)
    if not is_ready: raise gr.Error("L·ªói: Vui l√≤ng ch·ªçn ho·∫∑c t·∫£i l√™n m·ªôt gi·ªçng m·∫´u tr∆∞·ªõc khi t·∫°o gi·ªçng n√≥i.")
    if not gen_text.strip(): raise gr.Error("Vui l√≤ng nh·∫≠p n·ªôi dung vƒÉn b·∫£n.")
    
    try:
        text_from_ui = ref_text_from_ui.strip()
        
        if MODEL_TYPE == 'new':
            if text_from_ui and text_from_ui != tts_instance.ref_text:
                handle_preprocess(tts_instance.last_processed_audio_path, text_from_ui, clip_short=False, progress=progress)
        else:
            if text_from_ui and text_from_ui != ref_text_processed_old:
                handle_preprocess(ref_audio_path, text_from_ui, clip_short=False, progress=progress)
        
        print(f"B·∫Øt ƒë·∫ßu t·∫°o audio cho vƒÉn b·∫£n...")
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
            tmp_path = tmp_wav.name

        spectrogram_path = None
        if MODEL_TYPE == 'new':
            with suppress_outputs(PATH_TO_NEW_F5_REPO):
                from vinorm import TTSnorm
                final_text = TTSnorm(gen_text)
                tts_instance.generate(text=final_text, output_path=tmp_path, nfe_step=nfe_step, cfg_strength=cfg_strength, speed=speed, progress_callback=progress, target_rms=output_volume, cross_fade_duration=pause_duration)
        else: 
            with suppress_outputs(PATH_TO_OLD_F5_REPO):
                from vinorm import TTSnorm
                from f5_tts.infer.utils_infer import infer_process, save_spectrogram
                final_wave, final_sr, spectrogram = infer_process(ref_audio=ref_audio_path_old, ref_text=ref_text_processed_old, gen_text=TTSnorm(gen_text), model_obj=tts_instance['model'], vocoder=tts_instance['vocoder'], speed=speed, nfe_step=nfe_step, cfg_strength=cfg_strength, target_rms=output_volume, cross_fade_duration=pause_duration, progress=progress)
                sf.write(tmp_path, final_wave, final_sr)
                with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp_spec:
                    spectrogram_path = tmp_spec.name
                    save_spectrogram(spectrogram, spectrogram_path)

        final_wave, final_sr = sf.read(tmp_path)
        os.remove(tmp_path)

        output_path_from_ui = output_path_from_ui.strip()
        if output_path_from_ui:
            save_path = output_path_from_ui
            if not save_path.lower().endswith((".wav", ".mp3")): save_path += ".wav"
            parent_dir = os.path.dirname(save_path)
            if parent_dir and not os.path.exists(parent_dir): os.makedirs(parent_dir)
        else:
            default_dir = "outputs"
            os.makedirs(default_dir, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = os.path.join(default_dir, f"generated_audio_{timestamp}.wav")
        sf.write(save_path, final_wave, final_sr)
        gr.Info(f"‚úÖ √Çm thanh ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {save_path}")
        
        return (final_sr, final_wave), spectrogram_path
    except Exception as e:
        import traceback; traceback.print_exc()
        raise gr.Error(f"L·ªói khi t·∫°o gi·ªçng n√≥i: {e}")

# --- Giao di·ªán Gradio ---
with gr.Blocks(theme=latte) as demo:
    gr.Markdown("# üé§ F5-TTS: Vietnamese Text-to-Speech")
    
    with gr.Row():
        with gr.Column(scale=3):
            model_selector_dd = gr.Dropdown(choices=MODEL_DISPLAY_NAMES, value=MODEL_DISPLAY_NAMES[0], label="‚úÖ B∆∞·ªõc 1: Ch·ªçn Model b·∫°n mu·ªën s·ª≠ d·ª•ng", interactive=True)
        with gr.Column(scale=1):
            status_text = gr.Textbox(label="Tr·∫°ng th√°i Model", value="Ch∆∞a c√≥ model n√†o ƒë∆∞·ª£c t·∫£i.", interactive=False)

    gr.Markdown("---") 

    with gr.Row():
        sample_dropdown = gr.Dropdown(choices=sample_names, value=sample_names[0], label="üëá B∆∞·ªõc 2 (T√πy ch·ªçn): Ch·ªçn m·ªôt gi·ªçng m·∫´u c√≥ s·∫µn", interactive=True)
    
    with gr.Row():
        with gr.Column(scale=1):
            ref_audio_ui = gr.Audio(label="üîä B∆∞·ªõc 2: Ho·∫∑c t·∫£i l√™n gi·ªçng m·∫´u c·ªßa b·∫°n", type="filepath")
            ref_text_ui = gr.Textbox(label="üìù VƒÉn b·∫£n c·ªßa gi·ªçng m·∫´u", placeholder="N·ªôi dung audio m·∫´u s·∫Ω t·ª± ƒë·ªông xu·∫•t hi·ªán ·ªü ƒë√¢y.", lines=5, interactive=True)
        with gr.Column(scale=2):
            gen_text_ui = gr.Textbox(label="‚úçÔ∏è B∆∞·ªõc 3: Nh·∫≠p vƒÉn b·∫£n c·∫ßn t·∫°o gi·ªçng n√≥i", placeholder="Nh·∫≠p vƒÉn b·∫£n d√†i v√†o ƒë√¢y...", lines=11)
    
    with gr.Row():
        speed_slider = gr.Slider(0.3, 2.0, value=1.0, step=0.1, label="‚ö° T·ªëc ƒë·ªô")
        cfg_strength_slider = gr.Slider(0.5, 5.0, value=2.5, step=0.1, label="üó£Ô∏è ƒê·ªô b√°m s√°t gi·ªçng m·∫´u", info="Cao h∆°n = gi·ªëng gi·ªçng m·∫´u h∆°n. Th·∫•p h∆°n = t·ª± nhi√™n h∆°n.")
    
    with gr.Accordion("üõ†Ô∏è C√†i ƒë·∫∑t n√¢ng cao", open=False):
        nfe_step_slider = gr.Slider(minimum=16, maximum=64, value=32, step=2, label="üîç S·ªë b∆∞·ªõc kh·ª≠ nhi·ªÖu (NFE)", info="Cao h∆°n = ch·∫≠m h∆°n nh∆∞ng c√≥ th·ªÉ ch·∫•t l∆∞·ª£ng t·ªët h∆°n. Th·∫•p h∆°n = nhanh h∆°n.")
        output_volume_slider = gr.Slider(minimum=0.05, maximum=0.5, value=0.1, step=0.01, label="üîä √Çm l∆∞·ª£ng Output (RMS)", info="Ch·ªânh √¢m l∆∞·ª£ng t·ªïng th·ªÉ c·ªßa audio ƒë∆∞·ª£c t·∫°o ra.")
        pause_duration_slider = gr.Slider(minimum=0.0, maximum=1.0, value=0.15, step=0.05, label="‚è±Ô∏è ƒê·ªô d√†i ngh·ªâ gi·ªØa c√¢u (gi√¢y)", info="Th·ªùi gian n·ªëi √¢m gi·ªØa c√°c ƒëo·∫°n vƒÉn b·∫£n ƒë∆∞·ª£c chia nh·ªè.")
        output_path_ui = gr.Textbox(label="üéµ ƒê∆∞·ªùng d·∫´n l∆∞u file (t√πy ch·ªçn)", placeholder="ƒê·ªÉ tr·ªëng s·∫Ω t·ª± ƒë·ªông l∆∞u v√†o th∆∞ m·ª•c 'outputs'", value="")

    btn = gr.Button("üî• B∆∞·ªõc 4: T·∫°o gi·ªçng n√≥i", variant="primary")
    
    with gr.Row():
        output_audio = gr.Audio(label="üéß √Çm thanh t·∫°o ra", type="numpy")
        output_spectrogram = gr.Image(label="üìä Spectrogram", visible=False)

    def reset_to_upload():
        return None, "", sample_names[0]

    model_selector_dd.change(fn=load_and_prepare_model, inputs=[model_selector_dd], outputs=[status_text, ref_audio_ui, ref_text_ui, output_spectrogram], show_progress="full")
    sample_dropdown.change(fn=select_sample, inputs=[sample_dropdown], outputs=[ref_audio_ui, ref_text_ui], show_progress="full")
    ref_audio_ui.upload(fn=process_manual_upload, inputs=[ref_audio_ui], outputs=[ref_audio_ui, ref_text_ui, sample_dropdown], show_progress="full")
    ref_audio_ui.clear(fn=reset_to_upload, outputs=[ref_audio_ui, ref_text_ui, sample_dropdown])
    
    btn.click(
        fn=infer_tts, 
        inputs=[ref_audio_ui, ref_text_ui, gen_text_ui, speed_slider, cfg_strength_slider, nfe_step_slider, output_path_ui, output_volume_slider, pause_duration_slider], 
        outputs=[output_audio, output_spectrogram]
    )

demo.queue(max_size=20).launch(share=True)