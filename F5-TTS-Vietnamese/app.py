import spaces
import os
import sys
import gradio as gr
import json
import numpy as np
from datetime import datetime
from huggingface_hub import login
from cached_path import cached_path
import tempfile
import soundfile as sf
import contextlib
import torch # <<< DÃ’NG Bá»Š THIáº¾U ÄÃƒ ÄÆ¯á»¢C THÃŠM Láº I VÃ€O ÄÃ‚Y

from gradio.themes.utils import colors, fonts

# --- Theme ---
latte = gr.themes.Citrus(primary_hue=colors.yellow, secondary_hue=colors.rose, neutral_hue=colors.gray, font=fonts.GoogleFont("Inter"), font_mono=fonts.GoogleFont("JetBrains Mono")).set(body_background_fill="#eff1f5", body_text_color="#4c4f69", border_color_primary="#bcc0cc", block_background_fill="#ffffff", button_primary_background_fill="#df8e1d", button_primary_text_color="#ffffff", link_text_color="#dc8a78")

# --- Cáº¤U HÃŒNH MODEL ---
MODELS_DATA = {
    "Hynt_ViVoice": {"display_name": "Hynt ViVoice (1000h Ä‘á»c Tiáº¿ng Viá»‡t Vivoice)", "repo": "hynt/F5-TTS-Vietnamese-ViVoice", "model_file": "model_last.pt", "vocab_file": "config.json", "type": "old"},
    "Hynt_100h": {"display_name": "Hynt (150h Ä‘á»c Tiáº¿ng Viá»‡t 500k Steps)", "repo": "cuongdesign/Vietnamese-TTS", "model_file": "model_500000.pt", "vocab_file": "vocab.txt", "type": "old"},
    "DanhTran": {"display_name": "DanhTran (100h Ä‘á»c tiáº¿ng Viá»‡t dá»¯ liá»‡u cá»§a VinAI)", "repo": "danhtran2mind/vi-f5-tts", "model_file": "ckpts/model_last.pt", "vocab_file": "vocab.txt", "type": "old"},
    "ZaloPay": {"display_name": "ZaloPay (model cá»§a Zalopay 1tr3 Steps)", "repo": "zalopay/vietnamese-tts", "model_file": "model_1290000.pt", "vocab_file": "vocab.txt", "type": "old"},
    "EraX Smile Female": {"display_name": "EraX Smile Female (ChuyÃªn Clone cho giá»ng ná»¯ 8 vÃ¹ng miá»n)", "repo": "erax-ai/EraX-Smile-Female-F5-V1.0", "model_file": "model/model_612000.safetensors", "vocab_file": "model/vocab.txt", "type": "new", "init_params": {"vocoder_name": "vocos", "use_ema": False}},
    "EraX Smile Unisex": {"display_name": "EraX Smile Unisex (cáº£ Nam/Ná»¯ 8 vÃ¹ng miá»n)", "repo": "erax-ai/EraX-Smile-UnixSex-F5", "model_file": "models/overfit.safetensors", "vocab_file": "models/vocab.txt", "type": "new", "init_params": {"model_name": "F5TTS_v1_Base", "vocoder_name": "vocos", "use_ema": True, "target_sample_rate": 24000, "n_mel_channels": 100, "hop_length": 256, "win_length": 1024, "n_fft": 1024, "ode_method": 'euler'}},
}
MODEL_DISPLAY_NAMES = [info['display_name'] for info in MODELS_DATA.values()]
MODEL_DISPLAY_NAMES.insert(0, "(ChÆ°a chá»n model)")


# --- Táº£i Giá»ng Máº«u CÃ³ Sáºµn ---
SAMPLES_DIR = "samples"
SAMPLES_CONFIG = os.path.join(SAMPLES_DIR, "samples.json")
def load_samples():
    sample_names = ["(Tá»± táº£i lÃªn giá»ng cá»§a báº¡n)"]
    sample_lookup = {}
    if os.path.exists(SAMPLES_CONFIG):
        try:
            with open(SAMPLES_CONFIG, 'r', encoding='utf-8') as f:
                data = json.load(f)
            for sample in data:
                if 'name' in sample and 'audio_path' in sample and 'ref_text' in sample:
                    sample_names.append(sample['name'])
                    sample_lookup[sample['name']] = sample
            print(f"âœ… ÄÃ£ táº£i thÃ nh cÃ´ng {len(sample_lookup)} giá»ng máº«u.")
        except Exception as e:
            print(f"âš ï¸ Lá»—i khi Ä‘á»c file samples.json: {e}")
    else:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y file samples.json. Bá» qua viá»‡c táº£i giá»ng máº«u cÃ³ sáºµn.")
    return sample_names, sample_lookup
sample_names, sample_lookup = load_samples()

# --- BIáº¾N TOÃ€N Cá»¤C Äá»‚ QUáº¢N LÃ TRáº NG THÃI ---
PATH_TO_OLD_F5_REPO = os.path.abspath('f5_tts_old')
PATH_TO_NEW_F5_REPO = os.path.abspath('f5_tts_new')
tts_instance = None
MODEL_TYPE = None
ref_audio_path_old, ref_text_processed_old = None, None

# --- CÃC HÃ€M Xá»¬ LÃ LOGIC ---

@contextlib.contextmanager
def suppress_outputs(target_path):
    original_stdout, original_stderr = sys.stdout, sys.stderr
    sys.path.insert(0, target_path)
    try:
        with open(os.devnull, 'w') as devnull:
            sys.stdout, sys.stderr = devnull, devnull
            yield
    finally:
        sys.stdout, sys.stderr = original_stdout, original_stderr
        sys.path.pop(0)

@spaces.GPU
def load_and_prepare_model(selected_display_name, progress=gr.Progress()):
    progress(0, desc="Äang tÃ¬m thÃ´ng tin model...")
    global tts_instance, MODEL_TYPE, ref_audio_path_old, ref_text_processed_old

    if selected_display_name == "(ChÆ°a chá»n model)":
        tts_instance = None
        MODEL_TYPE = None
        return "Vui lÃ²ng chá»n má»™t model Ä‘á»ƒ báº¯t Ä‘áº§u.", None, "", gr.update(visible=False)

    tts_instance = None
    MODEL_TYPE = None
    ref_audio_path_old, ref_text_processed_old = None, None
    torch.cuda.empty_cache()

    try:
        model_key = next(key for key, info in MODELS_DATA.items() if info["display_name"] == selected_display_name)
        model_info = MODELS_DATA[model_key]
        
        MODEL_TYPE = model_info['type']
        
        progress(0.1, desc="Äang táº£i file tá»« Hugging Face Hub...")
        ckpt_path = str(cached_path(f"hf://{model_info['repo']}/{model_info['model_file']}"))
        vocab_path = str(cached_path(f"hf://{model_info['repo']}/{model_info['vocab_file']}"))
        
        progress(0.5, desc=f"Äang khá»Ÿi táº¡o model {selected_display_name}...")
        
        if MODEL_TYPE == 'old':
            print("Äang táº£i model theo kiáº¿n trÃºc F5-TTS Base...")
            with suppress_outputs(PATH_TO_OLD_F5_REPO):
                from f5_tts.model import DiT
                from f5_tts.infer.utils_infer import load_vocoder, load_model
                vocoder = load_vocoder()
                model = load_model(
                    model_cls=DiT, 
                    model_cfg=dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4),
                    ckpt_path=ckpt_path, vocab_file=vocab_path
                )
                tts_instance = {"model": model, "vocoder": vocoder}
        elif MODEL_TYPE == 'new':
            print(f"Äang táº£i model vá»›i cÃ¡c tham sá»‘: {model_info.get('init_params', {})}")
            with suppress_outputs(PATH_TO_NEW_F5_REPO):
                from f5tts_wrapper import F5TTSWrapper
                from safetensors.torch import load_file
                init_params = model_info.get('init_params', {})
                init_params['ckpt_path'] = ckpt_path
                init_params['vocab_file'] = vocab_path
                tts_instance = F5TTSWrapper(**init_params)
        
        progress(1, desc="Táº£i model thÃ nh cÃ´ng!")
        print(f"âœ… Táº£i model {selected_display_name} thÃ nh cÃ´ng.")
        
        return f"âœ… Model Ä‘Ã£ Ä‘Æ°á»£c táº£i: {selected_display_name}", None, "", gr.update(visible=(MODEL_TYPE == 'old'))
    
    except Exception as e:
        import traceback; traceback.print_exc()
        tts_instance, MODEL_TYPE = None, None
        error_msg = f"Lá»—i khi táº£i model: {e}"
        gr.Error(error_msg)
        return error_msg, None, "", gr.update(visible=False)

def handle_preprocess(audio_path, text, clip_short, progress):
    progress(0, desc="Äang xá»­ lÃ½ giá»ng máº«u...")
    gr.Info("ÄÃ£ nháº­n audio máº«u. Báº¯t Ä‘áº§u xá»­ lÃ½...")
    
    if MODEL_TYPE == 'new':
        with suppress_outputs(PATH_TO_NEW_F5_REPO):
            processed_path, transcribed_text = tts_instance.preprocess_reference(
                ref_audio_path=audio_path, ref_text=text, clip_short=clip_short
            )
    else:
        global ref_audio_path_old, ref_text_processed_old
        with suppress_outputs(PATH_TO_OLD_F5_REPO):
            from f5_tts.infer.utils_infer import preprocess_ref_audio_text
            processed_path, transcribed_text = preprocess_ref_audio_text(audio_path, text, clip_short=clip_short)
            ref_audio_path_old = processed_path
            ref_text_processed_old = transcribed_text
    
    progress(1, desc="Xá»­ lÃ½ giá»ng máº«u hoÃ n táº¥t!")
    gr.Info("Xá»­ lÃ½ giá»ng máº«u hoÃ n táº¥t!")
    print(f"Xá»­ lÃ½ giá»ng máº«u hoÃ n táº¥t.\n ÄÆ°á»ng dáº«n: {processed_path},\n VÄƒn báº£n cá»§a giá»ng máº«u: '{transcribed_text}'")
    return processed_path, transcribed_text

@spaces.GPU
def select_sample(sample_name, progress=gr.Progress()):
    if tts_instance is None:
        raise gr.Error("Vui lÃ²ng chá»n vÃ  táº£i má»™t model trÆ°á»›c khi chá»n giá»ng máº«u.")
    if not sample_name or sample_name == sample_names[0]: return None, ""
    sample_data = sample_lookup[sample_name]
    audio_path = os.path.join(SAMPLES_DIR, sample_data['audio_path'])
    return handle_preprocess(audio_path, sample_data['ref_text'], clip_short=False, progress=progress)

@spaces.GPU
def process_manual_upload(ref_audio_orig, progress=gr.Progress()):
    if tts_instance is None:
        raise gr.Error("Vui lÃ²ng chá»n vÃ  táº£i má»™t model trÆ°á»›c khi táº£i lÃªn giá»ng máº«u.")
    if not ref_audio_orig: return None, "", sample_names[0]
    processed_path, transcribed_text = handle_preprocess(ref_audio_orig, "", clip_short=True, progress=progress)
    return processed_path, transcribed_text, sample_names[0]

@spaces.GPU
def infer_tts(ref_audio_path, ref_text_from_ui, gen_text, speed, cfg_strength, nfe_step, output_path_from_ui, output_volume, pause_duration, progress=gr.Progress()):
    if tts_instance is None: raise gr.Error("Lá»—i: Vui lÃ²ng chá»n vÃ  táº£i má»™t model trÆ°á»›c khi táº¡o giá»ng nÃ³i.")
    is_ready = (tts_instance.ref_audio_processed is not None) if MODEL_TYPE == 'new' else (ref_audio_path_old is not None)
    if not is_ready: raise gr.Error("Lá»—i: Vui lÃ²ng chá»n hoáº·c táº£i lÃªn má»™t giá»ng máº«u trÆ°á»›c khi táº¡o giá»ng nÃ³i.")
    if not gen_text.strip(): raise gr.Error("Vui lÃ²ng nháº­p ná»™i dung vÄƒn báº£n.")
    
    try:
        text_from_ui = ref_text_from_ui.strip()
        
        if MODEL_TYPE == 'new':
            if text_from_ui and text_from_ui != tts_instance.ref_text:
                handle_preprocess(tts_instance.last_processed_audio_path, text_from_ui, clip_short=False, progress=progress)
        else:
            if text_from_ui and text_from_ui != ref_text_processed_old:
                handle_preprocess(ref_audio_path, text_from_ui, clip_short=False, progress=progress)
        
        print(f"Báº¯t Ä‘áº§u táº¡o audio cho vÄƒn báº£n...")
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
            tmp_path = tmp_wav.name

        spectrogram_path = None
        if MODEL_TYPE == 'new':
            with suppress_outputs(PATH_TO_NEW_F5_REPO):
                from vinorm import TTSnorm
                final_text = TTSnorm(gen_text)
                tts_instance.generate(text=final_text, output_path=tmp_path, nfe_step=nfe_step, cfg_strength=cfg_strength, speed=speed, progress_callback=progress, target_rms=output_volume, cross_fade_duration=pause_duration)
        else: 
            with suppress_outputs(PATH_TO_OLD_F5_REPO):
                from vinorm import TTSnorm
                from f5_tts.infer.utils_infer import infer_process, save_spectrogram
                final_wave, final_sr, spectrogram = infer_process(ref_audio=ref_audio_path_old, ref_text=ref_text_processed_old, gen_text=TTSnorm(gen_text), model_obj=tts_instance['model'], vocoder=tts_instance['vocoder'], speed=speed, nfe_step=nfe_step, cfg_strength=cfg_strength, target_rms=output_volume, cross_fade_duration=pause_duration, progress=progress)
                sf.write(tmp_path, final_wave, final_sr)
                with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp_spec:
                    spectrogram_path = tmp_spec.name
                    save_spectrogram(spectrogram, spectrogram_path)

        final_wave, final_sr = sf.read(tmp_path)
        os.remove(tmp_path)

        output_path_from_ui = output_path_from_ui.strip()
        if output_path_from_ui:
            save_path = output_path_from_ui
            if not save_path.lower().endswith((".wav", ".mp3")): save_path += ".wav"
            parent_dir = os.path.dirname(save_path)
            if parent_dir and not os.path.exists(parent_dir): os.makedirs(parent_dir)
        else:
            default_dir = "outputs"
            os.makedirs(default_dir, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = os.path.join(default_dir, f"generated_audio_{timestamp}.wav")
        sf.write(save_path, final_wave, final_sr)
        gr.Info(f"âœ… Ã‚m thanh Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {save_path}")
        
        return (final_sr, final_wave), spectrogram_path
    except Exception as e:
        import traceback; traceback.print_exc()
        raise gr.Error(f"Lá»—i khi táº¡o giá»ng nÃ³i: {e}")

# --- Giao diá»‡n Gradio ---
with gr.Blocks(theme=latte) as demo:
    gr.Markdown("# ðŸŽ¤ F5-TTS: Vietnamese Text-to-Speech")
    
    with gr.Row():
        with gr.Column(scale=3):
            model_selector_dd = gr.Dropdown(choices=MODEL_DISPLAY_NAMES, value=MODEL_DISPLAY_NAMES[0], label="âœ… BÆ°á»›c 1: Chá»n Model báº¡n muá»‘n sá»­ dá»¥ng", interactive=True)
        with gr.Column(scale=1):
            status_text = gr.Textbox(label="Tráº¡ng thÃ¡i Model", value="ChÆ°a cÃ³ model nÃ o Ä‘Æ°á»£c táº£i.", interactive=False)

    gr.Markdown("---") 

    with gr.Row():
        sample_dropdown = gr.Dropdown(choices=sample_names, value=sample_names[0], label="ðŸ‘‡ BÆ°á»›c 2 (TÃ¹y chá»n): Chá»n má»™t giá»ng máº«u cÃ³ sáºµn", interactive=True)
    
    with gr.Row():
        with gr.Column(scale=1):
            ref_audio_ui = gr.Audio(label="ðŸ”Š BÆ°á»›c 2: Hoáº·c táº£i lÃªn giá»ng máº«u cá»§a báº¡n", type="filepath")
            ref_text_ui = gr.Textbox(label="ðŸ“ VÄƒn báº£n cá»§a giá»ng máº«u", placeholder="Ná»™i dung audio máº«u sáº½ tá»± Ä‘á»™ng xuáº¥t hiá»‡n á»Ÿ Ä‘Ã¢y.", lines=5, interactive=True)
        with gr.Column(scale=2):
            gen_text_ui = gr.Textbox(label="âœï¸ BÆ°á»›c 3: Nháº­p vÄƒn báº£n cáº§n táº¡o giá»ng nÃ³i", placeholder="Nháº­p vÄƒn báº£n dÃ i vÃ o Ä‘Ã¢y...", lines=11)
    
    with gr.Row():
        speed_slider = gr.Slider(0.3, 2.0, value=1.0, step=0.1, label="âš¡ Tá»‘c Ä‘á»™")
        cfg_strength_slider = gr.Slider(0.5, 5.0, value=2.5, step=0.1, label="ðŸ—£ï¸ Äá»™ bÃ¡m sÃ¡t giá»ng máº«u", info="Cao hÆ¡n = giá»‘ng giá»ng máº«u hÆ¡n. Tháº¥p hÆ¡n = tá»± nhiÃªn hÆ¡n.")
    
    with gr.Accordion("ðŸ› ï¸ CÃ i Ä‘áº·t nÃ¢ng cao", open=False):
        nfe_step_slider = gr.Slider(minimum=16, maximum=64, value=32, step=2, label="ðŸ” Sá»‘ bÆ°á»›c khá»­ nhiá»…u (NFE)", info="Cao hÆ¡n = cháº­m hÆ¡n nhÆ°ng cÃ³ thá»ƒ cháº¥t lÆ°á»£ng tá»‘t hÆ¡n. Tháº¥p hÆ¡n = nhanh hÆ¡n.")
        output_volume_slider = gr.Slider(minimum=0.05, maximum=0.5, value=0.1, step=0.01, label="ðŸ”Š Ã‚m lÆ°á»£ng Output (RMS)", info="Chá»‰nh Ã¢m lÆ°á»£ng tá»•ng thá»ƒ cá»§a audio Ä‘Æ°á»£c táº¡o ra.")
        pause_duration_slider = gr.Slider(minimum=0.0, maximum=1.0, value=0.15, step=0.05, label="â±ï¸ Äá»™ dÃ i nghá»‰ giá»¯a cÃ¢u (giÃ¢y)", info="Thá»i gian ná»‘i Ã¢m giá»¯a cÃ¡c Ä‘oáº¡n vÄƒn báº£n Ä‘Æ°á»£c chia nhá».")
        output_path_ui = gr.Textbox(label="ðŸŽµ ÄÆ°á»ng dáº«n lÆ°u file (tÃ¹y chá»n)", placeholder="Äá»ƒ trá»‘ng sáº½ tá»± Ä‘á»™ng lÆ°u vÃ o thÆ° má»¥c 'outputs'", value="")

    btn = gr.Button("ðŸ”¥ BÆ°á»›c 4: Táº¡o giá»ng nÃ³i", variant="primary")
    
    with gr.Row():
        output_audio = gr.Audio(label="ðŸŽ§ Ã‚m thanh táº¡o ra", type="numpy")
        output_spectrogram = gr.Image(label="ðŸ“Š Spectrogram", visible=False)

    def reset_to_upload():
        return None, "", sample_names[0]

    model_selector_dd.change(fn=load_and_prepare_model, inputs=[model_selector_dd], outputs=[status_text, ref_audio_ui, ref_text_ui, output_spectrogram], show_progress="full")
    sample_dropdown.change(fn=select_sample, inputs=[sample_dropdown], outputs=[ref_audio_ui, ref_text_ui], show_progress="full")
    ref_audio_ui.upload(fn=process_manual_upload, inputs=[ref_audio_ui], outputs=[ref_audio_ui, ref_text_ui, sample_dropdown], show_progress="full")
    ref_audio_ui.clear(fn=reset_to_upload, outputs=[ref_audio_ui, ref_text_ui, sample_dropdown])
    
    btn.click(
        fn=infer_tts, 
        inputs=[ref_audio_ui, ref_text_ui, gen_text_ui, speed_slider, cfg_strength_slider, nfe_step_slider, output_path_ui, output_volume_slider, pause_duration_slider], 
        outputs=[output_audio, output_spectrogram]
    )

demo.queue(max_size=20).launch(share=True)