import spaces
import os
import sys
import gradio as gr
from huggingface_hub import login
from cached_path import cached_path
import tempfile
import soundfile as sf # C·∫ßn ƒë·ªÉ ƒë·ªçc file wav ƒë∆∞·ª£c t·∫°o ra

# --- PH·∫¶N 1: ƒê·ªåC C·∫§U H√åNH T·ª™ BI·∫æN M√îI TR∆Ø·ªúNG ---
MODEL_TYPE = os.getenv('MODEL_TYPE', 'new')
CKPT_PATH = os.getenv('CKPT_HF_PATH')
VOCAB_PATH = os.getenv('VOCAB_HF_PATH')
WORD_LIMIT = int(os.getenv('WORD_LIMIT', 1000))

# ƒê∆∞·ªùng d·∫´n ƒë·∫øn c√°c th∆∞ m·ª•c th∆∞ vi·ªán
PATH_TO_OLD_F5_REPO = os.path.abspath('f5_tts_old')
# ƒê∆∞·ªùng d·∫´n repo m·ªõi ph·∫£i tr·ªè ƒë·∫øn th∆∞ m·ª•c cha c·ªßa 'f5_tts' v√† 'f5tts_wrapper.py'
PATH_TO_NEW_F5_REPO = os.path.abspath('f5_tts_new')

if not CKPT_PATH or not VOCAB_PATH:
    raise ValueError("L·ªói: CKPT_PATH ho·∫∑c VOCAB_PATH ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p.")

# --- PH·∫¶N 2: T·∫¢I MODEL & CACHING LOGIC ---
tts_instance = None
last_ref_audio_path = None # Bi·∫øn cache ƒë·ªÉ l∆∞u ƒë∆∞·ªùng d·∫´n file m·∫´u cu·ªëi c√πng

print(f"Nh·∫≠n di·ªán lo·∫°i model: {MODEL_TYPE}")

if MODEL_TYPE == 'old':
    print("ƒêang t·∫£i model theo ki·∫øn tr√∫c F5-TTS Base...")
    sys.path.insert(0, PATH_TO_OLD_F5_REPO)
    from f5_tts.model import DiT
    from f5_tts.infer.utils_infer import load_vocoder, load_model
    
    vocoder = load_vocoder()
    model = load_model(
        DiT,
        dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4),
        ckpt_path=str(cached_path(CKPT_PATH)),
        vocab_file=str(cached_path(VOCAB_PATH)),
    )
    tts_instance = {"model": model, "vocoder": vocoder}
    sys.path.pop(0)
    print("‚úÖ T·∫£i model C≈® th√†nh c√¥ng.")

elif MODEL_TYPE == 'new':
    print("ƒêang t·∫£i model theo ki·∫øn tr√∫c F5-TTS v1...")
    sys.path.insert(0, PATH_TO_NEW_F5_REPO)
    from f5tts_wrapper import F5TTSWrapper

    tts_instance = F5TTSWrapper(
        vocoder_name="vocos",
        ckpt_path=str(cached_path(CKPT_PATH)),
        vocab_file=str(cached_path(VOCAB_PATH)),
        use_ema=False,
    )
    sys.path.pop(0)
    print("‚úÖ T·∫£i model M·ªöI th√†nh c√¥ng.")

def post_process(text):
    text = " " + text.replace('"', "") + " "
    text = text.replace(" . . ", " . ").replace(" .. ", " . ")
    text = text.replace(" , , ", " , ").replace(" ,, ", " , ")
    return " ".join(text.split())

@spaces.GPU
def infer_tts(ref_audio_orig: str, gen_text: str, speed: float = 1.0):
    global last_ref_audio_path # S·ª≠ d·ª•ng bi·∫øn cache to√†n c·ª•c
    if not ref_audio_orig:
        raise gr.Error("Vui l√≤ng t·∫£i l√™n m·ªôt t·ªáp √¢m thanh m·∫´u.")
    if not gen_text.strip():
        raise gr.Error("Vui l√≤ng nh·∫≠p n·ªôi dung vƒÉn b·∫£n.")
    if len(gen_text.strip().split()) > WORD_LIMIT:
        raise gr.Error(f"VƒÉn b·∫£n qu√° d√†i. Gi·ªõi h·∫°n l√† {WORD_LIMIT} t·ª´.")
    
    try:
        if MODEL_TYPE == 'old':
            # Logic cho model c≈© kh√¥ng thay ƒë·ªïi
            sys.path.insert(0, PATH_TO_OLD_F5_REPO)
            from vinorm import TTSnorm
            from f5_tts.infer.utils_infer import preprocess_ref_audio_text, infer_process, save_spectrogram

            final_text = post_process(TTSnorm(gen_text)).lower()
            ref_audio, ref_text = preprocess_ref_audio_text(ref_audio_orig, "")
            
            final_wave, final_sr, spectrogram = infer_process(
                ref_audio, ref_text.lower(), final_text, tts_instance["model"], tts_instance["vocoder"], speed=speed
            )
            
            with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp_file:
                save_spectrogram(spectrogram, tmp_file.name)
                spectrogram_path = tmp_file.name
            
            sys.path.pop(0)
            return (final_sr, final_wave), spectrogram_path

        elif MODEL_TYPE == 'new':
            sys.path.insert(0, PATH_TO_NEW_F5_REPO)
            from vinorm import TTSnorm
            
            # --- LOGIC CACHING CHO WRAPPER ---
            # Ki·ªÉm tra xem file m·∫´u c√≥ ph·∫£i l√† file m·ªõi kh√¥ng
            if ref_audio_orig != last_ref_audio_path:
                print(f"Ph√°t hi·ªán √¢m thanh m·∫´u m·ªõi. ƒêang x·ª≠ l√Ω: {ref_audio_orig}")
                # N·∫øu m·ªõi, ch·∫°y preprocess_reference
                # Wrapper n√†y kh√¥ng c·∫ßn vƒÉn b·∫£n tham chi·∫øu, n√≥ c√≥ th·ªÉ t·ª± d√πng Whisper
                tts_instance.preprocess_reference(
                    ref_audio_path=ref_audio_orig,
                    ref_text="", # ƒê·ªÉ tr·ªëng ƒë·ªÉ wrapper t·ª± d√πng ASR
                    clip_short=True
                )
                # C·∫≠p nh·∫≠t cache
                last_ref_audio_path = ref_audio_orig
                print("X·ª≠ l√Ω gi·ªçng m·∫´u th√†nh c√¥ng.")
            else:
                print("S·ª≠ d·ª•ng gi·ªçng m·∫´u ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥.")

            final_text = post_process(TTSnorm(gen_text)).lower()
            
            # Wrapper l∆∞u file tr·ª±c ti·∫øp, n√™n ta c·∫ßn t·∫°o file t·∫°m
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
                output_path = tmp_wav.name

            # Ch·∫°y generate, n√≥ s·∫Ω l∆∞u k·∫øt qu·∫£ v√†o output_path
            tts_instance.generate(
                text=final_text,
                output_path=output_path,
                speed=speed
            )
            
            # ƒê·ªçc l·∫°i file wav v·ª´a t·∫°o ƒë·ªÉ tr·∫£ v·ªÅ cho Gradio
            final_sr, final_wave = sf.read(output_path)
            
            sys.path.pop(0)
            # Wrapper kh√¥ng tr·∫£ v·ªÅ spectrogram, n√™n ta tr·∫£ v·ªÅ None
            return (final_sr, final_wave), None

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise gr.Error(f"L·ªói khi t·∫°o gi·ªçng n√≥i: {e}")

# --- Giao di·ªán Gradio kh√¥ng ƒë·ªïi ---
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# üé§ F5-TTS: Vietnamese Text-to-Speech")
    display_name = os.getenv('CKPT_HF_PATH', 'N/A').split('/')[1]
    gr.Markdown(f"### Model: **{display_name}** | Ki·∫øn tr√∫c: **{MODEL_TYPE.upper()}** | Gi·ªõi h·∫°n: **{WORD_LIMIT} t·ª´**")

    with gr.Row():
        ref_audio = gr.Audio(label="üîä √Çm thanh m·∫´u", type="filepath")
        gen_text = gr.Textbox(label="üìù VƒÉn b·∫£n", placeholder="Nh·∫≠p vƒÉn b·∫£n...", lines=3)
    
    speed = gr.Slider(0.3, 2.0, value=1.0, step=0.1, label="‚ö° T·ªëc ƒë·ªô")
    btn = gr.Button("üî• T·∫°o gi·ªçng n√≥i", variant="primary")
    
    with gr.Row():
        output_audio = gr.Audio(label="üéß √Çm thanh t·∫°o ra", type="numpy")
        output_spectrogram = gr.Image(label="üìä Spectrogram (Ch·ªâ c√≥ ·ªü model c≈©)")

    btn.click(infer_tts, inputs=[ref_audio, gen_text, speed], outputs=[output_audio, output_spectrogram])

demo.queue().launch(share=True)