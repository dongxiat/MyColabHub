import spaces
import os
import sys
import gradio as gr
import json
import numpy as np
from datetime import datetime
from huggingface_hub import login
from cached_path import cached_path
import tempfile
import soundfile as sf
import contextlib

from gradio.themes.utils import colors, fonts

# --- Theme ---
latte = gr.themes.Citrus(primary_hue=colors.yellow, secondary_hue=colors.rose, neutral_hue=colors.gray, font=fonts.GoogleFont("Inter"), font_mono=fonts.GoogleFont("JetBrains Mono")).set(body_background_fill="#eff1f5", body_text_color="#4c4f69", border_color_primary="#bcc0cc", block_background_fill="#ffffff", button_primary_background_fill="#df8e1d", button_primary_text_color="#ffffff", link_text_color="#dc8a78")

# --- T·∫£i Gi·ªçng M·∫´u C√≥ S·∫µn ---
SAMPLES_DIR = "samples"
SAMPLES_CONFIG = os.path.join(SAMPLES_DIR, "samples.json")

def load_samples():
    sample_names = ["(T·ª± t·∫£i l√™n gi·ªçng c·ªßa b·∫°n)"]
    sample_lookup = {}
    if os.path.exists(SAMPLES_CONFIG):
        try:
            with open(SAMPLES_CONFIG, 'r', encoding='utf-8') as f:
                data = json.load(f)
            for sample in data:
                if 'name' in sample and 'audio_path' in sample and 'ref_text' in sample:
                    sample_names.append(sample['name'])
                    sample_lookup[sample['name']] = sample
            print(f"‚úÖ ƒê√£ t·∫£i th√†nh c√¥ng {len(sample_lookup)} gi·ªçng m·∫´u.")
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi ƒë·ªçc file samples.json: {e}")
    else:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file samples.json. B·ªè qua vi·ªác t·∫£i gi·ªçng m·∫´u c√≥ s·∫µn.")
    return sample_names, sample_lookup

sample_names, sample_lookup = load_samples()

# --- T·∫£i Model ---
MODEL_TYPE = os.getenv('MODEL_TYPE', 'new')
CKPT_PATH = os.getenv('CKPT_HF_PATH')
VOCAB_PATH = os.getenv('VOCAB_HF_PATH')
WORD_LIMIT = int(os.getenv('WORD_LIMIT', 1000))
DISPLAY_NAME = os.getenv('DISPLAY_NAME', 'Unknown Model')
INIT_PARAMS_JSON = os.getenv('INIT_PARAMS_JSON', '{}')
INIT_PARAMS = json.loads(INIT_PARAMS_JSON)

PATH_TO_OLD_F5_REPO = os.path.abspath('f5_tts_old')
PATH_TO_NEW_F5_REPO = os.path.abspath('f5_tts_new')

tts_instance = None
print(f"Nh·∫≠n di·ªán lo·∫°i model: {MODEL_TYPE}")

if MODEL_TYPE == 'old':
    print("ƒêang t·∫£i model theo ki·∫øn tr√∫c F5-TTS Base...")
    sys.path.insert(0, PATH_TO_OLD_F5_REPO)
    from f5_tts.model import DiT
    from f5_tts.infer.utils_infer import load_vocoder, load_model
    vocoder = load_vocoder()
    model = load_model(
        model_cls=DiT, 
        model_cfg=dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4),
        ckpt_path=str(cached_path(CKPT_PATH)), 
        vocab_file=str(cached_path(VOCAB_PATH)),
    )
    tts_instance = {"model": model, "vocoder": vocoder}
    sys.path.pop(0)
    print("‚úÖ T·∫£i model C≈® th√†nh c√¥ng.")
elif MODEL_TYPE == 'new':
    print(f"ƒêang t·∫£i model v·ªõi c√°c tham s·ªë: {INIT_PARAMS}")
    sys.path.insert(0, PATH_TO_NEW_F5_REPO)
    from f5tts_wrapper import F5TTSWrapper
    from safetensors.torch import load_file
    INIT_PARAMS['ckpt_path'] = str(cached_path(CKPT_PATH))
    INIT_PARAMS['vocab_file'] = str(cached_path(VOCAB_PATH))
    tts_instance = F5TTSWrapper(**INIT_PARAMS)
    sys.path.pop(0)
    print("‚úÖ T·∫£i model M·ªöI th√†nh c√¥ng.")

ref_audio_path_old, ref_text_processed_old = None, None

# --- C√ÅC H√ÄM X·ª¨ L√ù LOGIC ---
@contextlib.contextmanager
def suppress_outputs(target_path):
    original_stdout, original_stderr = sys.stdout, sys.stderr
    sys.path.insert(0, target_path)
    try:
        with open(os.devnull, 'w') as devnull:
            sys.stdout, sys.stderr = devnull, devnull
            yield
    finally:
        sys.stdout, sys.stderr = original_stdout, original_stderr
        sys.path.pop(0)

def handle_preprocess(audio_path, text, clip_short, progress):
    progress(0, desc="ƒêang x·ª≠ l√Ω gi·ªçng m·∫´u...")
    gr.Info("ƒê√£ nh·∫≠n audio m·∫´u. B·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")
    
    if MODEL_TYPE == 'new':
        with suppress_outputs(PATH_TO_NEW_F5_REPO):
            processed_path, transcribed_text = tts_instance.preprocess_reference(
                ref_audio_path=audio_path, ref_text=text, clip_short=clip_short
            )
    else: # Model 'old'
        global ref_audio_path_old, ref_text_processed_old
        with suppress_outputs(PATH_TO_OLD_F5_REPO):
            from f5_tts.infer.utils_infer import preprocess_ref_audio_text
            processed_path, transcribed_text = preprocess_ref_audio_text(audio_path, text, clip_short=clip_short)
            ref_audio_path_old = processed_path
            ref_text_processed_old = transcribed_text
    
    progress(1, desc="X·ª≠ l√Ω gi·ªçng m·∫´u ho√†n t·∫•t!")
    gr.Info("X·ª≠ l√Ω gi·ªçng m·∫´u ho√†n t·∫•t!")
    print(f"X·ª≠ l√Ω gi·ªçng m·∫´u ho√†n t·∫•t.\n ƒê∆∞·ªùng d·∫´n: {processed_path},\n VƒÉn b·∫£n c·ªßa gi·ªçng m·∫´u: '{transcribed_text}'")
    return processed_path, transcribed_text

@spaces.GPU
def select_sample(sample_name, progress=gr.Progress()):
    if not sample_name or sample_name == sample_names[0]:
        return None, ""
    sample_data = sample_lookup[sample_name]
    audio_path = os.path.join(SAMPLES_DIR, sample_data['audio_path'])
    return handle_preprocess(audio_path, sample_data['ref_text'], clip_short=False, progress=progress)

@spaces.GPU
def process_manual_upload(ref_audio_orig, progress=gr.Progress()):
    if not ref_audio_orig: return None, "", sample_names[0]
    processed_path, transcribed_text = handle_preprocess(ref_audio_orig, "", clip_short=True, progress=progress)
    return processed_path, transcribed_text, sample_names[0]

@spaces.GPU
def infer_tts(ref_audio_path, ref_text_from_ui, gen_text, speed, cfg_strength, nfe_step, progress=gr.Progress()):
    is_ready = (tts_instance.ref_audio_processed is not None) if MODEL_TYPE == 'new' else (ref_audio_path_old is not None)
    if not is_ready:
        raise gr.Error("L·ªói: Vui l√≤ng ch·ªçn ho·∫∑c t·∫£i l√™n m·ªôt gi·ªçng m·∫´u tr∆∞·ªõc khi t·∫°o gi·ªçng n√≥i.")
    if not gen_text.strip():
        raise gr.Error("Vui l√≤ng nh·∫≠p n·ªôi dung vƒÉn b·∫£n.")
    
    try:
        text_from_ui = ref_text_from_ui.strip()
        
        if MODEL_TYPE == 'new':
            if text_from_ui and text_from_ui != tts_instance.ref_text:
                handle_preprocess(tts_instance.last_processed_audio_path, text_from_ui, clip_short=False, progress=progress)
        else:
            if text_from_ui and text_from_ui != ref_text_processed_old:
                handle_preprocess(ref_audio_path, text_from_ui, clip_short=False, progress=progress)
        
        print(f"B·∫Øt ƒë·∫ßu t·∫°o audio cho vƒÉn b·∫£n...")
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
            tmp_path = tmp_wav.name

        spectrogram_path = None
        if MODEL_TYPE == 'new':
            with suppress_outputs(PATH_TO_NEW_F5_REPO):
                from vinorm import TTSnorm
                final_text = TTSnorm(gen_text)
                tts_instance.generate(text=final_text, output_path=tmp_path, nfe_step=nfe_step, cfg_strength=cfg_strength, speed=speed, progress_callback=progress)
        else:
            with suppress_outputs(PATH_TO_OLD_F5_REPO):
                from vinorm import TTSnorm
                from f5_tts.infer.utils_infer import infer_process, save_spectrogram
                final_wave, final_sr, spectrogram = infer_process(
                    ref_audio=ref_audio_path_old, 
                    ref_text=ref_text_processed_old, 
                    gen_text=TTSnorm(gen_text),
                    model_obj=tts_instance['model'], 
                    vocoder=tts_instance['vocoder'], 
                    speed=speed, 
                    nfe_step=nfe_step,
                    progress=progress # <<< S·ª¨A L·ªñI: TRUY·ªÄN ƒê√öNG ƒê·ªêI T∆Ø·ª¢NG PROGRESS
                )
                sf.write(tmp_path, final_wave, final_sr)
                with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp_spec:
                    spectrogram_path = tmp_spec.name
                    save_spectrogram(spectrogram, spectrogram_path)

        final_wave, final_sr = sf.read(tmp_path)
        os.remove(tmp_path)
        
        return (final_sr, final_wave), spectrogram_path
    except Exception as e:
        import traceback; traceback.print_exc()
        raise gr.Error(f"L·ªói khi t·∫°o gi·ªçng n√≥i: {e}")

# --- Giao di·ªán Gradio ---
with gr.Blocks(theme=latte) as demo:
    gr.Markdown("# üé§ F5-TTS: Vietnamese Text-to-Speech")
    gr.Markdown(f"### Model: **{DISPLAY_NAME}** | Ki·∫øn tr√∫c: **{MODEL_TYPE.upper()}** | Gi·ªõi h·∫°n: **{WORD_LIMIT} t·ª´**")

    with gr.Row():
        sample_dropdown = gr.Dropdown(choices=sample_names, value=sample_names[0], label="üëá Ch·ªçn m·ªôt gi·ªçng m·∫´u c√≥ s·∫µn", interactive=True)
    
    with gr.Row():
        with gr.Column(scale=1):
            ref_audio_ui = gr.Audio(label="1. T·∫£i l√™n ho·∫∑c ch·ªçn √¢m thanh m·∫´u", type="filepath")
            ref_text_ui = gr.Textbox(label="2. VƒÉn b·∫£n c·ªßa gi·ªçng m·∫´u", placeholder="N·ªôi dung audio m·∫´u s·∫Ω t·ª± ƒë·ªông xu·∫•t hi·ªán ·ªü ƒë√¢y.", lines=5, interactive=True)
        with gr.Column(scale=2):
            gen_text_ui = gr.Textbox(label="3. Nh·∫≠p vƒÉn b·∫£n c·∫ßn t·∫°o gi·ªçng n√≥i", placeholder="Nh·∫≠p vƒÉn b·∫£n d√†i v√†o ƒë√¢y...", lines=11)
    
    with gr.Row():
        speed_slider = gr.Slider(0.3, 2.0, value=1.0, step=0.1, label="‚ö° T·ªëc ƒë·ªô")
        cfg_strength_slider = gr.Slider(0.5, 5.0, value=2.5, step=0.1, label="üó£Ô∏è ƒê·ªô b√°m s√°t gi·ªçng m·∫´u")
    
    with gr.Accordion("üõ†Ô∏è C√†i ƒë·∫∑t n√¢ng cao", open=False):
        nfe_step_slider = gr.Slider(minimum=16, maximum=64, value=32, step=2, label="üîç S·ªë b∆∞·ªõc kh·ª≠ nhi·ªÖu (NFE)", info="Cao h∆°n = ch·∫≠m h∆°n nh∆∞ng c√≥ th·ªÉ ch·∫•t l∆∞·ª£ng t·ªët h∆°n. Th·∫•p h∆°n = nhanh h∆°n.")

    btn = gr.Button("üî• 4. T·∫°o gi·ªçng n√≥i", variant="primary")
    
    with gr.Row():
        output_audio = gr.Audio(label="üéß √Çm thanh t·∫°o ra", type="numpy")
        output_spectrogram = gr.Image(label="üìä Spectrogram (Ch·ªâ c√≥ ·ªü model c≈©)", visible=(MODEL_TYPE == 'old'))

    def reset_to_upload():
        return None, "", sample_names[0]

    sample_dropdown.change(fn=select_sample, inputs=[sample_dropdown], outputs=[ref_audio_ui, ref_text_ui], show_progress="full")
    ref_audio_ui.upload(fn=process_manual_upload, inputs=[ref_audio_ui], outputs=[ref_audio_ui, ref_text_ui, sample_dropdown], show_progress="full")
    ref_audio_ui.clear(fn=reset_to_upload, outputs=[ref_audio_ui, ref_text_ui, sample_dropdown])
    
    btn.click(
        fn=infer_tts, 
        inputs=[ref_audio_ui, ref_text_ui, gen_text_ui, speed_slider, cfg_strength_slider, nfe_step_slider], 
        outputs=[output_audio, output_spectrogram]
    )

demo.queue().launch()