import spaces
import os
import sys
import gradio as gr
import json
import re
import numpy as np
from datetime import datetime
from huggingface_hub import login
from cached_path import cached_path
import tempfile
import soundfile as sf
import contextlib # C·∫ßn ƒë·ªÉ t·∫Øt ti·∫øng log th∆∞ vi·ªán

# --- PH·∫¶N 1 & 2: KH√îNG THAY ƒê·ªîI ---
MODEL_TYPE = os.getenv('MODEL_TYPE', 'new')
CKPT_PATH = os.getenv('CKPT_HF_PATH')
VOCAB_PATH = os.getenv('VOCAB_HF_PATH')
WORD_LIMIT = int(os.getenv('WORD_LIMIT', 1000))
DISPLAY_NAME = os.getenv('DISPLAY_NAME', 'Unknown Model')
INIT_PARAMS_JSON = os.getenv('INIT_PARAMS_JSON', '{}')
INIT_PARAMS = json.loads(INIT_PARAMS_JSON)

PATH_TO_OLD_F5_REPO = os.path.abspath('f5_tts_old')
PATH_TO_NEW_F5_REPO = os.path.abspath('f5_tts_new')

if not CKPT_PATH or not VOCAB_PATH:
    raise ValueError("L·ªói: CKPT_PATH ho·∫∑c VOCAB_PATH ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p.")

tts_instance = None
last_ref_audio_path = None

print(f"Nh·∫≠n di·ªán lo·∫°i model: {MODEL_TYPE}")

# ... (Logic t·∫£i model kh√¥ng ƒë·ªïi) ...
if MODEL_TYPE == 'old':
    print("ƒêang t·∫£i model theo ki·∫øn tr√∫c F5-TTS Base...")
    sys.path.insert(0, PATH_TO_OLD_F5_REPO)
    from f5_tts.model import DiT
    from f5_tts.infer.utils_infer import load_vocoder, load_model
    vocoder = load_vocoder()
    model = load_model(
        DiT, dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4),
        ckpt_path=str(cached_path(CKPT_PATH)), vocab_file=str(cached_path(VOCAB_PATH)),
    )
    tts_instance = {"model": model, "vocoder": vocoder}
    sys.path.pop(0)
    print("‚úÖ T·∫£i model C≈® th√†nh c√¥ng.")
elif MODEL_TYPE == 'new':
    print(f"ƒêang t·∫£i model v·ªõi c√°c tham s·ªë: {INIT_PARAMS}")
    sys.path.insert(0, PATH_TO_NEW_F5_REPO)
    from f5tts_wrapper import F5TTSWrapper
    INIT_PARAMS['ckpt_path'] = str(cached_path(CKPT_PATH))
    INIT_PARAMS['vocab_file'] = str(cached_path(VOCAB_PATH))
    tts_instance = F5TTSWrapper(**INIT_PARAMS)
    sys.path.pop(0)
    print("‚úÖ T·∫£i model M·ªöI th√†nh c√¥ng.")

def intelligent_chunker(text: str, min_words=3, max_words=25):
    # (H√†m n√†y kh√¥ng ƒë·ªïi)
    sentences = re.split(r'([.!?‚Ä¶])', text)
    sentences = [sentences[i] + (sentences[i+1] if i+1 < len(sentences) else '') for i in range(0, len(sentences), 2)]
    sentences = [s.strip() for s in sentences if s.strip()]
    if not sentences: return []
    final_chunks, processed_chunks = [], []
    for s in sentences:
        if len(s.split()) < min_words and final_chunks: final_chunks[-1] += " " + s
        else: final_chunks.append(s)
    for chunk in final_chunks:
        if len(chunk.split()) > max_words:
            parts = [p.strip() for p in chunk.split(',')]
            buffer = ""
            for part in parts:
                if not buffer: buffer = part
                elif len(buffer.split()) + len(part.split()) <= max_words: buffer += ", " + part
                else:
                    processed_chunks.append(buffer)
                    buffer = part
            if buffer: processed_chunks.append(buffer)
        else: processed_chunks.append(chunk)
    return [c for c in processed_chunks if c]

@spaces.GPU
def infer_tts(ref_audio_orig: str, gen_text: str, speed: float, cfg_strength: float, nfe_step: int, progress=gr.Progress()):
    global last_ref_audio_path
    if not ref_audio_orig: raise gr.Error("Vui l√≤ng t·∫£i l√™n m·ªôt t·ªáp √¢m thanh m·∫´u.")
    if not gen_text.strip(): raise gr.Error("Vui l√≤ng nh·∫≠p n·ªôi dung vƒÉn b·∫£n.")
    if len(gen_text.strip().split()) > WORD_LIMIT: raise gr.Error(f"VƒÉn b·∫£n qu√° d√†i. Gi·ªõi h·∫°n l√† {WORD_LIMIT} t·ª´.")
    
    try:
        # <<< THAY ƒê·ªîI 1: X·ª≠ l√Ω gi·ªçng m·∫´u TR∆Ø·ªöC v√≤ng l·∫∑p >>>
        ref_audio_processed, ref_text_processed = None, None
        if ref_audio_orig != last_ref_audio_path:
            progress(0, desc="Ph√°t hi·ªán gi·ªçng m·∫´u m·ªõi. ƒêang x·ª≠ l√Ω...")
            if MODEL_TYPE == 'new':
                sys.path.insert(0, PATH_TO_NEW_F5_REPO)
                tts_instance.preprocess_reference(ref_audio_path=ref_audio_orig, ref_text="", clip_short=True)
                sys.path.pop(0)
                print(f"X·ª≠ l√Ω gi·ªçng m·∫´u (m·ªõi) th√†nh c√¥ng.")
            else: # MODEL_TYPE == 'old'
                sys.path.insert(0, PATH_TO_OLD_F5_REPO)
                from f5_tts.infer.utils_infer import preprocess_ref_audio_text
                # T·∫Øt ti·∫øng m·ªôt l·∫ßn ƒë·ªÉ kh√¥ng hi·ªán log x·ª≠ l√Ω ref audio
                with open(os.devnull, 'w') as devnull, contextlib.redirect_stdout(devnull), contextlib.redirect_stderr(devnull):
                    ref_audio_processed, ref_text_processed = preprocess_ref_audio_text(ref_audio_orig, "")
                sys.path.pop(0)
                # In ra log s·∫°ch theo √Ω b·∫°n
                print(f"X·ª≠ l√Ω gi·ªçng m·∫´u (c≈©) th√†nh c√¥ng.")
                print(f"ref_text: {ref_text_processed}")

            last_ref_audio_path = ref_audio_orig

        print("ƒêang t√°ch vƒÉn b·∫£n th√†nh c√°c chunk th√¥ng minh...")
        sentences = intelligent_chunker(gen_text)
        if not sentences: raise gr.Error("Kh√¥ng t√¨m th·∫•y c√¢u h·ª£p l·ªá n√†o trong vƒÉn b·∫£n.")
        print(f"ƒê√£ t√°ch th√†nh {len(sentences)} chunk. B·∫Øt ƒë·∫ßu t·∫°o audio...")

        audio_chunks = []
        final_sr = 24000
        
        for i, sentence in enumerate(sentences):
            progress(i / len(sentences), desc=f"ƒêang t·∫°o chunk {i+1}/{len(sentences)}")
            # In ra log theo format b·∫°n mu·ªën
            print(f"gen_text {i}: {sentence[:80]}...")
            
            if MODEL_TYPE == 'old':
                # T·∫Øt ti·∫øng h√†m infer_process ƒë·ªÉ log ƒë∆∞·ª£c s·∫°ch
                with open(os.devnull, 'w') as devnull, contextlib.redirect_stdout(devnull), contextlib.redirect_stderr(devnull):
                    sys.path.insert(0, PATH_TO_OLD_F5_REPO)
                    from vinorm import TTSnorm
                    from f5_tts.infer.utils_infer import infer_process
                    final_text = TTSnorm(sentence).lower()
                    # S·ª≠ d·ª•ng l·∫°i ref audio ƒë√£ x·ª≠ l√Ω, kh√¥ng c·∫ßn g·ªçi l·∫°i preprocess_ref_audio_text
                    wave, sr, _ = infer_process(ref_audio_processed, ref_text_processed.lower(), final_text, tts_instance["model"], tts_instance["vocoder"], speed=speed, nfe_step=nfe_step)
                    sys.path.pop(0)
            else: 
                sys.path.insert(0, PATH_TO_NEW_F5_REPO)
                from vinorm import TTSnorm
                final_text = TTSnorm(sentence)
                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
                    tmp_path = tmp_wav.name
                tts_instance.generate(
                    text=final_text, output_path=tmp_path, nfe_step=nfe_step,
                    cfg_strength=cfg_strength, speed=speed, sway_sampling_coef=-1, cross_fade_duration=0.15
                )
                wave, sr = sf.read(tmp_path)
                os.remove(tmp_path)
                sys.path.pop(0)

            audio_chunks.append(wave)
            final_sr = sr 

        print("ƒêang gh√©p c√°c file audio...")
        progress(1, desc="ƒêang gh√©p n·ªëi v√† ho√†n t·∫•t...")
        full_audio = np.concatenate(audio_chunks)

        output_dir = "output"
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        final_output_path = os.path.join(output_dir, f"generated_audio_{timestamp}.wav")
        sf.write(final_output_path, full_audio, final_sr)
        print(f"‚úÖ √Çm thanh ho√†n ch·ªânh ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {final_output_path}")

        yield (final_sr, full_audio), None

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise gr.Error(f"L·ªói khi t·∫°o gi·ªçng n√≥i: {e}")

# --- Giao di·ªán Gradio (Kh√¥ng thay ƒë·ªïi) ---
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# üé§ F5-TTS: Vietnamese Text-to-Speech")
    gr.Markdown(f"### Model: **{DISPLAY_NAME}** | Ki·∫øn tr√∫c: **{MODEL_TYPE.upper()}** | Gi·ªõi h·∫°n: **{WORD_LIMIT} t·ª´**")
    with gr.Row():
        ref_audio = gr.Audio(label="üîä √Çm thanh m·∫´u", type="filepath")
        gen_text = gr.Textbox(label="üìù VƒÉn b·∫£n", placeholder="Nh·∫≠p vƒÉn b·∫£n d√†i v√†o ƒë√¢y...", lines=5)
    with gr.Row():
        speed = gr.Slider(0.3, 2.0, value=1.0, step=0.1, label="‚ö° T·ªëc ƒë·ªô")
        cfg_strength_slider = gr.Slider(0.5, 5.0, value=2.5, step=0.1, label="üó£Ô∏è ƒê·ªô b√°m s√°t gi·ªçng m·∫´u")
    with gr.Accordion("üõ†Ô∏è C√†i ƒë·∫∑t n√¢ng cao", open=False):
        nfe_step_slider = gr.Slider(
            minimum=16, maximum=64, value=32, step=2,
            label="üîç S·ªë b∆∞·ªõc kh·ª≠ nhi·ªÖu (NFE)",
            info="Cao h∆°n = ch·∫≠m h∆°n nh∆∞ng c√≥ th·ªÉ ch·∫•t l∆∞·ª£ng t·ªët h∆°n. Th·∫•p h∆°n = nhanh h∆°n."
        )
    btn = gr.Button("üî• T·∫°o gi·ªçng n√≥i", variant="primary")
    with gr.Row():
        output_audio = gr.Audio(label="üéß √Çm thanh t·∫°o ra", type="numpy")
        output_spectrogram = gr.Image(label="üìä Spectrogram (Ch·ªâ c√≥ ·ªü model c≈©)")
    btn.click(
        infer_tts, 
        inputs=[ref_audio, gen_text, speed, cfg_strength_slider, nfe_step_slider], 
        outputs=[output_audio, output_spectrogram]
    )

demo.queue().launch(share=True)