import spaces
import os
import sys
import gradio as gr
import json
import re
import numpy as np
from datetime import datetime
from huggingface_hub import login
from cached_path import cached_path
import tempfile
import soundfile as sf
import contextlib

# --- PH·∫¶N 1 & 2: KH√îNG THAY ƒê·ªîI ---
MODEL_TYPE = os.getenv('MODEL_TYPE', 'new')
CKPT_PATH = os.getenv('CKPT_HF_PATH')
VOCAB_PATH = os.getenv('VOCAB_HF_PATH')
WORD_LIMIT = int(os.getenv('WORD_LIMIT', 1000))
DISPLAY_NAME = os.getenv('DISPLAY_NAME', 'Unknown Model')
INIT_PARAMS_JSON = os.getenv('INIT_PARAMS_JSON', '{}')
INIT_PARAMS = json.loads(INIT_PARAMS_JSON)

PATH_TO_OLD_F5_REPO = os.path.abspath('f5_tts_old')
PATH_TO_NEW_F5_REPO = os.path.abspath('f5_tts_new')

if not CKPT_PATH or not VOCAB_PATH:
    raise ValueError("L·ªói: CKPT_PATH ho·∫∑c VOCAB_PATH ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p.")

tts_instance = None
print(f"Nh·∫≠n di·ªán lo·∫°i model: {MODEL_TYPE}")

# ... (Logic t·∫£i model kh√¥ng ƒë·ªïi) ...
if MODEL_TYPE == 'old':
    print("ƒêang t·∫£i model theo ki·∫øn tr√∫c F5-TTS Base...")
    sys.path.insert(0, PATH_TO_OLD_F5_REPO)
    from f5_tts.model import DiT
    from f5_tts.infer.utils_infer import load_vocoder, load_model, chunk_text
    vocoder = load_vocoder()
    model = load_model(
        DiT, dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4),
        ckpt_path=str(cached_path(CKPT_PATH)), vocab_file=str(cached_path(VOCAB_PATH)),
    )
    tts_instance = {"model": model, "vocoder": vocoder}
    sys.path.pop(0)
    print("‚úÖ T·∫£i model C≈® th√†nh c√¥ng.")
elif MODEL_TYPE == 'new':
    print(f"ƒêang t·∫£i model v·ªõi c√°c tham s·ªë: {INIT_PARAMS}")
    sys.path.insert(0, PATH_TO_NEW_F5_REPO)
    from f5tts_wrapper import F5TTSWrapper
    INIT_PARAMS['ckpt_path'] = str(cached_path(CKPT_PATH))
    INIT_PARAMS['vocab_file'] = str(cached_path(VOCAB_PATH))
    tts_instance = F5TTSWrapper(**INIT_PARAMS)
    sys.path.pop(0)
    print("‚úÖ T·∫£i model M·ªöI th√†nh c√¥ng.")

# <<< C√ÅC BI·∫æN CACHE TO√ÄN C·ª§C >>>
last_processed_ref_path = None
last_processed_ref_text = None

@spaces.GPU
def process_reference_audio(ref_audio_orig, progress=gr.Progress()):
    global last_processed_ref_path, last_processed_ref_text
    if not ref_audio_orig: return None, ""
    
    progress(0, desc="ƒêang x·ª≠ l√Ω gi·ªçng m·∫´u...")
    gr.Info("ƒê√£ nh·∫≠n audio m·∫´u. B·∫Øt ƒë·∫ßu x·ª≠ l√Ω (c·∫Øt v√† phi√™n √¢m)...")
    
    clipped_audio_path = ref_audio_orig
    transcribed_text = ""

    if MODEL_TYPE == 'new':
        with open(os.devnull, 'w') as devnull, contextlib.redirect_stdout(devnull), contextlib.redirect_stderr(devnull):
            sys.path.insert(0, PATH_TO_NEW_F5_REPO)
            _, transcribed_text = tts_instance.preprocess_reference(
                ref_audio_path=ref_audio_orig, ref_text="", clip_short=True)
            processed_audio_tensor = tts_instance.ref_audio_processed
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
                clipped_audio_path = tmp_wav.name
            sf.write(clipped_audio_path, processed_audio_tensor.squeeze().cpu().numpy(), tts_instance.target_sample_rate)
            sys.path.pop(0)
    else:
        with open(os.devnull, 'w') as devnull, contextlib.redirect_stdout(devnull), contextlib.redirect_stderr(devnull):
            sys.path.insert(0, PATH_TO_OLD_F5_REPO)
            from f5_tts.infer.utils_infer import preprocess_ref_audio_text
            _, transcribed_text = preprocess_ref_audio_text(ref_audio_orig, "")
            sys.path.pop(0)

    # C·∫≠p nh·∫≠t cache sau khi x·ª≠ l√Ω
    last_processed_ref_path = clipped_audio_path
    last_processed_ref_text = transcribed_text
    
    progress(1, desc="X·ª≠ l√Ω gi·ªçng m·∫´u ho√†n t·∫•t!")
    gr.Info("X·ª≠ l√Ω gi·ªçng m·∫´u ho√†n t·∫•t! Vui l√≤ng ki·ªÉm tra v√† s·ª≠a l·∫°i vƒÉn b·∫£n phi√™n √¢m n·∫øu c·∫ßn.")
    print(f"X·ª≠ l√Ω gi·ªçng m·∫´u ho√†n t·∫•t. VƒÉn b·∫£n nh·∫≠n d·∫°ng: '{transcribed_text}'")
    
    return clipped_audio_path, transcribed_text

@spaces.GPU
def infer_tts(ref_audio_ui: str, ref_text_ui: str, gen_text: str, speed: float, cfg_strength: float, nfe_step: int, force_reprocess: bool, progress=gr.Progress()):
    global last_processed_ref_path, last_processed_ref_text
    if not ref_audio_ui: raise gr.Error("L·ªói: Kh√¥ng t√¨m th·∫•y √¢m thanh m·∫´u. Vui l√≤ng t·∫£i l√™n.")
    if not gen_text.strip(): raise gr.Error("Vui l√≤ng nh·∫≠p n·ªôi dung vƒÉn b·∫£n.")
    
    try:
        # <<< LOGIC X·ª¨ L√ù L·∫†I GI·ªåNG M·∫™U ƒê√É S·ª¨A L·ªñI >>>
        # ƒêi·ªÅu ki·ªán ƒë·ªÉ x·ª≠ l√Ω l·∫°i:
        # 1. Ng∆∞·ªùi d√πng tick v√†o √¥ "B·∫Øt bu·ªôc x·ª≠ l√Ω l·∫°i".
        # 2. Audio hi·ªán t·∫°i tr√™n UI kh√°c v·ªõi audio ƒë√£ cache.
        # 3. Text tr√™n UI kh√°c v·ªõi text ƒë√£ cache.
        should_reprocess = (
            force_reprocess or
            ref_audio_ui != last_processed_ref_path or
            (ref_text_ui.strip() != last_processed_ref_text and ref_text_ui.strip() != "")
        )

        if should_reprocess:
            progress(0, desc="Ph√°t hi·ªán thay ƒë·ªïi ho·∫∑c y√™u c·∫ßu. ƒêang x·ª≠ l√Ω l·∫°i gi·ªçng m·∫´u...")
            
            # L·∫•y vƒÉn b·∫£n t·ª´ UI ƒë·ªÉ x·ª≠ l√Ω l·∫°i. N·∫øu tr·ªëng th√¨ m·ªõi ch·∫°y ASR.
            ref_text_to_use = ref_text_ui.strip()
            print(f"ƒêang x·ª≠ l√Ω l·∫°i gi·ªçng m·∫´u v·ªõi vƒÉn b·∫£n: '{ref_text_to_use if ref_text_to_use else 'S·∫Ω ch·∫°y ASR'}'")

            if MODEL_TYPE == 'new':
                with open(os.devnull, 'w') as devnull, contextlib.redirect_stdout(devnull), contextlib.redirect_stderr(devnull):
                    sys.path.insert(0, PATH_TO_NEW_F5_REPO)
                    # Truy·ªÅn th·∫≥ng vƒÉn b·∫£n ƒë√£ s·ª≠a v√†o ƒë√¢y
                    tts_instance.preprocess_reference(ref_audio_path=ref_audio_ui, ref_text=ref_text_to_use, clip_short=True)
                    sys.path.pop(0)
            
            # C·∫≠p nh·∫≠t l·∫°i cache sau khi x·ª≠ l√Ω l·∫°i
            last_processed_ref_path = ref_audio_ui
            last_processed_ref_text = ref_text_ui.strip()
            print("X·ª≠ l√Ω l·∫°i gi·ªçng m·∫´u ho√†n t·∫•t.")
        else:
            print("S·ª≠ d·ª•ng gi·ªçng m·∫´u ƒë√£ ƒë∆∞·ª£c cache. B·ªè qua b∆∞·ªõc x·ª≠ l√Ω l·∫°i.")

        # --- Ph·∫ßn t·∫°o audio ---
        print(f"B·∫Øt ƒë·∫ßu t·∫°o audio cho to√†n b·ªô vƒÉn b·∫£n...")
        
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
            tmp_path = tmp_wav.name

        if MODEL_TYPE == 'old':
            sys.path.insert(0, PATH_TO_OLD_F5_REPO)
            from vinorm import TTSnorm
            from f5_tts.infer.utils_infer import infer_process, chunk_text, preprocess_ref_audio_text
            # Lu√¥n d√πng text t·ª´ UI
            ref_audio_processed, ref_text_processed = preprocess_ref_audio_text(ref_audio_ui, ref_text_ui.strip())
            sentences = chunk_text(TTSnorm(gen_text).lower())
            audio_chunks = []
            final_sr = 24000
            for i, sentence in enumerate(sentences):
                progress(i / len(sentences), desc=f"ƒêang t·∫°o chunk {i+1}/{len(sentences)}")
                with open(os.devnull, 'w') as devnull, contextlib.redirect_stdout(devnull), contextlib.redirect_stderr(devnull):
                    wave, sr, _ = infer_process(ref_audio_processed, ref_text_processed.lower(), sentence, tts_instance["model"], tts_instance["vocoder"], speed=speed, nfe_step=nfe_step)
                audio_chunks.append(wave)
                final_sr = sr
            full_audio = np.concatenate(audio_chunks)
            sf.write(tmp_path, full_audio, final_sr)
            sys.path.pop(0)
        else:
            with open(os.devnull, 'w') as devnull, contextlib.redirect_stdout(devnull), contextlib.redirect_stderr(devnull):
                sys.path.insert(0, PATH_TO_NEW_F5_REPO)
                from vinorm import TTSnorm
                final_text = TTSnorm(gen_text)
                tts_instance.generate(text=final_text, output_path=tmp_path, nfe_step=nfe_step,
                    cfg_strength=cfg_strength, speed=speed, sway_sampling_coef=-1, cross_fade_duration=0.15)
                sys.path.pop(0)

        progress(0.9, desc="ƒêang ƒë·ªçc k·∫øt qu·∫£...")
        final_wave, final_sr = sf.read(tmp_path)
        os.remove(tmp_path)
        
        output_dir = "output"
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        final_output_path = os.path.join(output_dir, f"generated_audio_{timestamp}.wav")
        sf.write(final_output_path, final_wave, final_sr)
        print(f"‚úÖ √Çm thanh ho√†n ch·ªânh ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {final_output_path}")
        
        progress(1, desc="Ho√†n th√†nh!")
        return (final_sr, final_wave), None

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise gr.Error(f"L·ªói khi t·∫°o gi·ªçng n√≥i: {e}")

# --- GIAO DI·ªÜN GRADIO V·ªöI 1 N√öT B·∫§M V√Ä T·ª∞ ƒê·ªòNG X·ª¨ L√ù ---
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# üé§ F5-TTS: Vietnamese Text-to-Speech")
    gr.Markdown(f"### Model: **{DISPLAY_NAME}** | Ki·∫øn tr√∫c: **{MODEL_TYPE.upper()}** | Gi·ªõi h·∫°n: **{WORD_LIMIT} t·ª´**")
    with gr.Row():
        with gr.Column(scale=1):
            ref_audio_ui = gr.Audio(label="1. T·∫£i l√™n √¢m thanh m·∫´u", type="filepath")
            ref_text_ui = gr.Textbox(
                label="2. S·ª≠a l·∫°i vƒÉn b·∫£n (n·∫øu c·∫ßn)",
                placeholder="N·ªôi dung audio m·∫´u s·∫Ω t·ª± ƒë·ªông xu·∫•t hi·ªán ·ªü ƒë√¢y sau khi b·∫°n t·∫£i file l√™n.",
                lines=5, interactive=True)
        with gr.Column(scale=2):
            gen_text_ui = gr.Textbox(label="3. Nh·∫≠p vƒÉn b·∫£n c·∫ßn t·∫°o gi·ªçng n√≥i", placeholder="Nh·∫≠p vƒÉn b·∫£n d√†i v√†o ƒë√¢y...", lines=11)
    
    with gr.Row():
        speed_slider = gr.Slider(0.3, 2.0, value=1.0, step=0.1, label="‚ö° T·ªëc ƒë·ªô")
        cfg_strength_slider = gr.Slider(0.5, 5.0, value=2.5, step=0.1, label="üó£Ô∏è ƒê·ªô b√°m s√°t gi·ªçng m·∫´u")
    
    with gr.Accordion("üõ†Ô∏è C√†i ƒë·∫∑t n√¢ng cao", open=False):
        nfe_step_slider = gr.Slider(
            minimum=16, maximum=64, value=32, step=2,
            label="üîç S·ªë b∆∞·ªõc kh·ª≠ nhi·ªÖu (NFE)",
            info="Cao h∆°n = ch·∫≠m h∆°n nh∆∞ng c√≥ th·ªÉ ch·∫•t l∆∞·ª£ng t·ªët h∆°n. Th·∫•p h∆°n = nhanh h∆°n.")
        force_reprocess_ui = gr.Checkbox(
            label="B·∫Øt bu·ªôc x·ª≠ l√Ω l·∫°i gi·ªçng m·∫´u", value=False,
            info="Tick v√†o √¥ n√†y n·∫øu b·∫°n mu·ªën model h·ªçc l·∫°i gi·ªçng m·∫´u t·ª´ ƒë·∫ßu, h·ªØu √≠ch khi k·∫øt qu·∫£ b·ªã l·ªói.")
    
    btn = gr.Button("üî• 4. T·∫°o gi·ªçng n√≥i", variant="primary")
    
    with gr.Row():
        output_audio = gr.Audio(label="üéß √Çm thanh t·∫°o ra", type="numpy")
        output_spectrogram = gr.Image(label="üìä Spectrogram (Ch·ªâ c√≥ ·ªü model c≈©)")

    # G·∫Øn s·ª± ki·ªán upload ƒë·ªÉ x·ª≠ l√Ω t·ª± ƒë·ªông
    ref_audio_ui.upload(
        process_reference_audio,
        inputs=[ref_audio_ui],
        outputs=[ref_audio_ui, ref_text_ui]
    )
    
    # G·∫Øn s·ª± ki·ªán click cho n√∫t t·∫°o gi·ªçng n√≥i
    btn.click(
        infer_tts, 
        inputs=[ref_audio_ui, ref_text_ui, gen_text_ui, speed_slider, cfg_strength_slider, nfe_step_slider, force_reprocess_ui], 
        outputs=[output_audio, output_spectrogram]
    )

demo.queue().launch(share=True)